# Current Task: Fix Real-Time Token Streaming

## Problem Description

The chat with the LLM currently generates the response correctly, but the frontend does not refresh token by token during generation. Instead, the entire response is displayed only after all tokens have been generated.

**Expected Behavior**: Tokens should appear in the UI one by one (or in small batches) as they are generated by the model, providing a smooth streaming experience.

**Actual Behavior**: The UI waits until the entire response is complete before displaying anything.

## INVESTIGATION STATUS: IN PROGRESS

### Latest Findings (Updated)

**Debug logging added to trace the token streaming path:**

1. Added logging to `src/web/websocket.rs` (handle_websocket spawn block)
2. Added logging to `src/web/chat/generation.rs` (generate_llama_response and token_sender.send)

### Key Discovery: Two Separate Endpoints

The application has **TWO different chat endpoints**:

1. **HTTP POST `/api/chat`** (Non-streaming)
   - Used by: Direct HTTP requests (e.g., curl to port 8000)
   - `token_sender = None` - tokens are NOT streamed
   - Response returned only after ALL tokens generated
   - **This is what curl tests hit**

2. **WebSocket `/ws/chat/stream`** (Streaming)
   - Used by: Frontend via `TauriAPI.sendMessageStream()`
   - `token_sender = Some(tx)` - tokens SHOULD stream in real-time
   - Each token sent via channel as it's generated
   - **Frontend should use this via port 4000 (Vite proxy)**

### Debug Logs Show:

```
[GENERATION] generate_llama_response called, token_sender is None
[GENERATION] Conversation ID: chat_2025-12-15-04-59-31-982.txt
[GENERATION] About to start token generation loop. token_sender is NONE
[GENERATION] WARNING: token_sender is None, tokens not being streamed!
```

This confirms the HTTP endpoint (`/api/chat`) was hit, NOT the WebSocket endpoint.

### Vite Proxy Configuration (vite.config.ts)

```typescript
proxy: {
  '/api': {
    target: 'http://localhost:8000',
    changeOrigin: true,
  },
  '/ws': {
    target: 'http://localhost:8000',
    changeOrigin: true,
    ws: true, // Enable WebSocket proxying
  },
}
```

**WebSocket proxying IS configured** but needs to be verified.

## Architecture

### Two Streaming Paths

1. **Direct streaming** (`/ws/chat/stream` → `handle_websocket` → `token_sender.send()`)
   - Frontend opens WebSocket connection
   - Sends chat request as JSON
   - Receives tokens one-by-one via WebSocket messages
   - **Status: NOT TESTED YET**

2. **Conversation watcher** (`/ws/conversation/watch/` → `broadcast_streaming_update`)
   - Frontend subscribes to conversation file updates
   - Receives full conversation content on each update
   - **Status: WORKING (but shows full content, not incremental)**

### Frontend Flow (useChat.ts)

```typescript
// Line 204: Uses WebSocket streaming
await TauriAPI.sendMessageStream(
    request,
    // onToken - called for each token generated
    (token, tokenCount, maxTokenCount) => {
        setMessages(prev => {
            const lastMsg = prev[prev.length - 1];
            if (lastMsg && lastMsg.id === assistantMessageId) {
                return [...prev.slice(0, -1), { ...lastMsg, content: lastMsg.content + token }];
            }
            return prev;
        });
    },
    // onComplete, onError...
);
```

### Backend Flow (websocket.rs)

```rust
// Create channel for streaming tokens
let (tx, mut rx) = mpsc::unbounded_channel::<TokenData>();

// Spawn generation task with token sender
tokio::spawn(async move {
    if let Some(state) = state_clone {
        match generate_llama_response(&message, state, conversation_logger, Some(tx), false).await {
            // ...
        }
    }
});

// Stream tokens through WebSocket
loop {
    tokio::select! {
        token_result = rx.recv() => {
            match token_result {
                Some(token_data) => {
                    let json = serde_json::json!({"type": "token", "token": token_data.token, ...});
                    ws_sender.send(WsMessage::Text(json.to_string())).await;
                }
                // ...
            }
        }
    }
}
```

## Next Steps

1. **Test via Frontend (port 4000)**
   - Open http://localhost:4000 in browser
   - Send a chat message
   - Check backend logs for `[WS_CHAT]` entries
   - Check if tokens appear one-by-one in the UI

2. **If WebSocket Not Working**
   - Check browser DevTools Network tab for WebSocket connection
   - Verify Vite proxy is correctly forwarding WebSocket upgrade
   - Look for any WebSocket connection errors

3. **If WebSocket Works But Still No Streaming**
   - Check if `token_sender.send()` is being called (debug logs)
   - Check if tokens are arriving at frontend (console logs)
   - Verify React state updates are happening correctly

## Files Modified

1. `src/web/websocket.rs` - Added debug logging to spawn block
2. `src/web/chat/generation.rs` - Added debug logging for token_sender
3. `src/hooks/useChat.ts` - Added flushSync (from earlier fix attempt)

## Previous Attempts

### flushSync Fix (Not the Root Cause)
Added `flushSync` from `react-dom` to force synchronous state update before streaming starts.
This was NOT the root cause - the real issue is whether WebSocket streaming is working at all.

## Servers Running

- **Backend**: http://localhost:8000 (cargo run --bin llama_chat_web)
- **Frontend**: http://localhost:4000 (npm run dev / Vite)

## How to Test

1. Open http://localhost:4000 in a browser
2. Load a model if not already loaded
3. Send a message in the chat
4. Watch the backend terminal for logs
5. Expected: See `[WS_CHAT]` and `[GENERATION] Token #X sent via channel` logs
6. Check if tokens stream in real-time in the UI
