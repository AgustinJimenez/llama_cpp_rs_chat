// Simple web server version of LLaMA Chat (without Tauri)
use std::net::SocketAddr;
use std::convert::Infallible;
use std::sync::{Arc, Mutex};
use std::num::NonZeroU32;
use std::fs;
use std::io;
use serde::{Deserialize, Serialize};
use serde_json;
use gguf_llms::{GgufHeader, GgufReader, Value};
use std::io::BufReader;
use tokio::sync::mpsc;
use hyper::body::Bytes;

// HTTP server using hyper
use hyper::service::{make_service_fn, service_fn};
use hyper::{Body, Method, Request, Response, Server, StatusCode};

// LLaMA integration
#[cfg(feature = "docker")]
use llama_cpp_2::{
    context::params::LlamaContextParams,
    llama_backend::LlamaBackend,
    llama_batch::LlamaBatch,
    model::{params::LlamaModelParams, AddBos, LlamaModel, Special, LlamaChatMessage},
    sampling::LlamaSampler,
    // send_logs_to_tracing, LogOptions,
};

// Configuration structure
#[derive(Deserialize, Serialize, Clone)]
struct SamplerConfig {
    sampler_type: String,
    temperature: f64,
    top_p: f64,
    top_k: u32,
    mirostat_tau: f64,
    mirostat_eta: f64,
    model_path: Option<String>,
    system_prompt: Option<String>,
    context_size: Option<u32>,
    stop_tokens: Option<Vec<String>>,
    #[serde(default)]
    model_history: Vec<String>,
}

// Token data with metadata for streaming
#[derive(Serialize, Clone)]
struct TokenData {
    token: String,
    tokens_used: i32,
    max_tokens: i32,
}

// Common stop tokens for different model providers
fn get_common_stop_tokens() -> Vec<String> {
    vec![
        // Custom command tag - stop AFTER the command is complete
        "</COMMAND>".to_string(),

        // LLaMA 3+ tokens
        "<|end_of_text|>".to_string(),
        "<|eot_id|>".to_string(),
        "<|start_header_id|>".to_string(),
        "<|end_header_id|>".to_string(),

        // Qwen tokens
        "<|im_start|>".to_string(),
        "<|im_end|>".to_string(),
        "<|endoftext|>".to_string(),

        // Mistral/Mixtral tokens
        "[INST]".to_string(),
        "[/INST]".to_string(),
        "</s>".to_string(),

        // Phi tokens
        "<|user|>".to_string(),
        "<|assistant|>".to_string(),
        "<|end|>".to_string(),
        "<|system|>".to_string(),

        // Gemma tokens
        "<start_of_turn>".to_string(),
        "<end_of_turn>".to_string(),

        // Generic role tokens
        "<|start_of_role|>".to_string(),
        "<|end_of_role|>".to_string(),
    ]
}

impl Default for SamplerConfig {
    fn default() -> Self {
        // Default system prompt from test.rs
        let default_system_prompt = r#"
You are a local cli AI tool with shell access on a computer, your goal is to understand what the user wants and help with tasks.
The current system is running on your detected OS
From that, you must automatically know what commands are available and how to format them

Rules of operation
- Don't ask the user to do tasks you can do
- You can freely manipulate files or folders for normal work.
- Try at least 10 times to do the tasks with a different approach before requesting more information to the user if you are stuck 
- Confirm only for risky changes (for example, deleting or overwriting many files, running privileged commands, installing software, or altering system paths).
- Before working with a file, verify that it exists first
- When looking for files: if not found in current directory, immediately use: find . -name "*filename*" -type f
- For file searches: use wildcards to match partial names across the entire project (e.g., find . -name "*alejandro*" -type f)
- IMPORTANT: Always put wildcards in quotes when using find command (e.g., "*.gguf" not *.gguf)
- NEVER search the entire filesystem with find / - use specific directories like . or ~/
- After finding file location, navigate and read the file from its actual path
- Always check subdirectories that seem relevant to the file you're looking for
- Always be thorough - execute search commands, don't just describe them
- Summarize the output briefly after execution and what you think about it.
- If a command fails, show the error and try a different approach - don't repeat the same failing command
- For web access, use curl, wget, or PowerShell's Invoke-WebRequest, with short timeouts and limited output.
- Keep responses concise, technical, and neutral.
- Try to run commands without moving from the current directory, don't use the 'cd' command
- Don't repeat the same commands over and over again

To run a command, use this exact format:
<COMMAND>command_here</COMMAND>
"#;

        Self {
            sampler_type: "Greedy".to_string(),
            temperature: 0.7,
            top_p: 0.95,
            top_k: 20,
            mirostat_tau: 5.0,
            mirostat_eta: 0.1,
            model_path: Some("/app/models/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf".to_string()),
            system_prompt: Some(default_system_prompt.trim().to_string()),
            context_size: Some(32768),
            stop_tokens: Some(get_common_stop_tokens()),
            model_history: Vec::new(),
        }
    }
}

// Request/Response structures
#[derive(Deserialize)]
struct ChatRequest {
    message: String,
    conversation_id: Option<String>,
}

#[derive(Serialize)]
struct ChatResponse {
    message: ChatMessage,
    conversation_id: String,
    tokens_used: Option<i32>,
    max_tokens: Option<i32>,
}

#[derive(Serialize)]
struct ChatMessage {
    id: String,
    role: String,
    content: String,
    timestamp: u64,
}

#[derive(Serialize)]
struct ConversationFile {
    name: String,
    display_name: String,
    timestamp: String,
}

#[derive(Serialize)]
struct ConversationsResponse {
    conversations: Vec<ConversationFile>,
}

#[derive(Serialize)]
struct ConversationContentResponse {
    content: String,
    messages: Vec<ChatMessage>,
}

#[derive(Serialize)]
struct FileItem {
    name: String,
    path: String,
    is_directory: bool,
    size: Option<u64>,
}

#[derive(Serialize)]
struct BrowseFilesResponse {
    files: Vec<FileItem>,
    current_path: String,
    parent_path: Option<String>,
}

#[derive(Serialize)]
struct ModelStatus {
    loaded: bool,
    model_path: Option<String>,
    last_used: Option<String>,
    memory_usage_mb: Option<u64>,
}

#[derive(Deserialize)]
struct ModelLoadRequest {
    model_path: String,
}

#[derive(Serialize)]
struct ModelResponse {
    success: bool,
    message: String,
    status: Option<ModelStatus>,
}

// Shared state for LLaMA
#[cfg(feature = "docker")]
struct LlamaState {
    backend: LlamaBackend,
    model: Option<LlamaModel>,
    current_model_path: Option<String>,
    model_context_length: Option<u32>,
    chat_template_type: Option<String>, // Store detected template type
    last_used: std::time::SystemTime,
}

#[cfg(feature = "docker")]
type SharedLlamaState = Arc<Mutex<Option<LlamaState>>>;

type SharedConversationLogger = Arc<Mutex<ConversationLogger>>;

// Helper function to load configuration
fn load_config() -> SamplerConfig {
    let config_path = "assets/config.json";
    match fs::read_to_string(config_path) {
        Ok(content) => {
            match serde_json::from_str::<SamplerConfig>(&content) {
                Ok(config) => config,
                Err(e) => {
                    eprintln!("Failed to parse config file: {}, using defaults", e);
                    SamplerConfig::default()
                }
            }
        }
        Err(_) => {
            // Config file doesn't exist, use defaults
            SamplerConfig::default()
        }
    }
}

// Helper function to add a model path to history
fn add_to_model_history(model_path: &str) {
    let config_path = "assets/config.json";

    // Load current config
    let mut config = load_config();

    // Remove the path if it already exists (to move it to the front)
    config.model_history.retain(|p| p != model_path);

    // Add to the front of the list
    config.model_history.insert(0, model_path.to_string());

    // Keep only the last 10 paths
    if config.model_history.len() > 10 {
        config.model_history.truncate(10);
    }

    // Save the updated config
    let _ = fs::create_dir_all("assets");
    if let Ok(json) = serde_json::to_string_pretty(&config) {
        let _ = fs::write(config_path, json);
    }
}

// Helper function to parse command with proper quote handling
fn parse_command_with_quotes(cmd: &str) -> Vec<String> {
    let mut parts = Vec::new();
    let mut current_part = String::new();
    let mut in_quotes = false;
    let mut chars = cmd.chars().peekable();

    while let Some(ch) = chars.next() {
        match ch {
            '"' => {
                in_quotes = !in_quotes;
                // Don't include the quote character in the output
            }
            ' ' if !in_quotes => {
                if !current_part.is_empty() {
                    parts.push(current_part.clone());
                    current_part.clear();
                }
            }
            _ => {
                current_part.push(ch);
            }
        }
    }

    if !current_part.is_empty() {
        parts.push(current_part);
    }

    parts
}

// Helper function to execute system commands
fn execute_command(cmd: &str) -> String {
    use std::process::Command;
    use std::env;

    // Parse command with proper quote handling
    let parts = parse_command_with_quotes(cmd.trim());
    if parts.is_empty() {
        return "Error: Empty command".to_string();
    }

    let command_name = &parts[0];

    // Basic command validation - reject obviously invalid commands
    if command_name.len() < 2 || command_name.contains("/") && !command_name.starts_with("/") {
        return format!("Error: Invalid command format: {}", command_name);
    }

    // Prevent dangerous filesystem-wide searches
    if command_name == "find" && parts.len() > 1 {
        let search_path = &parts[1];
        if search_path == "/" || search_path == "/usr" || search_path == "/System" {
            return format!("Error: Filesystem-wide searches are not allowed for performance and security reasons. Try searching in specific directories like current directory '.'");
        }
    }

    // Special handling for cd command - actually change the process working directory
    if command_name == "cd" {
        let target_dir = if parts.len() > 1 {
            &parts[1]
        } else {
            return "Error: cd command requires a directory argument".to_string();
        };

        match env::set_current_dir(target_dir) {
            Ok(_) => {
                if let Ok(new_dir) = env::current_dir() {
                    format!("Successfully changed directory to: {}", new_dir.display())
                } else {
                    "Directory changed successfully".to_string()
                }
            }
            Err(e) => {
                format!("Error: Failed to change directory: {}", e)
            }
        }
    } else {
        // Normal command execution for non-cd commands
        let mut command = Command::new(&parts[0]);
        if parts.len() > 1 {
            command.args(&parts[1..]);
        }

        match command.output() {
            Ok(output) => {
                let stdout = String::from_utf8_lossy(&output.stdout);
                let stderr = String::from_utf8_lossy(&output.stderr);

                // Handle commands that succeed silently
                if output.status.success() && stdout.is_empty() && stderr.is_empty() {
                    match command_name.as_str() {
                        "find" => "No files found matching the search criteria".to_string(),
                        "mkdir" => "Directory created successfully".to_string(),
                        "touch" => "File created successfully".to_string(),
                        "rm" | "rmdir" => "File/directory removed successfully".to_string(),
                        "mv" | "cp" => "File operation completed successfully".to_string(),
                        "chmod" => "Permissions changed successfully".to_string(),
                        _ => {
                            if parts.len() > 1 {
                                format!("Command '{}' executed successfully", parts.join(" "))
                            } else {
                                format!("Command '{}' executed successfully", command_name)
                            }
                        }
                    }
                } else if !stderr.is_empty() {
                    format!("{}\nError: {}", stdout, stderr)
                } else {
                    stdout.to_string()
                }
            }
            Err(e) => {
                format!("Failed to execute command: {}", e)
            }
        }
    }
}

// Helper function to get model status
#[cfg(feature = "docker")]
fn get_model_status(llama_state: &SharedLlamaState) -> ModelStatus {
    match llama_state.lock() {
        Ok(state_guard) => {
            match state_guard.as_ref() {
                Some(state) => {
                    let loaded = state.model.is_some();
                    let model_path = state.current_model_path.clone();
                    let last_used = state.last_used
                        .duration_since(std::time::UNIX_EPOCH)
                        .ok()
                        .map(|d| d.as_secs().to_string());
                    
                    ModelStatus {
                        loaded,
                        model_path,
                        last_used,
                        memory_usage_mb: if loaded { Some(512) } else { None }, // Rough estimate
                    }
                }
                None => ModelStatus {
                    loaded: false,
                    model_path: None,
                    last_used: None,
                    memory_usage_mb: None,
                }
            }
        }
        Err(_) => ModelStatus {
            loaded: false,
            model_path: None,
            last_used: None,
            memory_usage_mb: None,
        }
    }
}

// Helper function to calculate optimal GPU layers based on available VRAM
#[cfg(feature = "docker")]
fn calculate_optimal_gpu_layers(model_path: &str) -> u32 {
    use std::fs;

    // Get model file size to estimate memory requirements
    let model_size_bytes = match fs::metadata(model_path) {
        Ok(metadata) => metadata.len(),
        Err(_) => {
            println!("[GPU] Could not read model file size, defaulting to 32 layers");
            return 32;
        }
    };

    let model_size_gb = model_size_bytes as f64 / (1024.0 * 1024.0 * 1024.0);
    println!("[GPU] Model file size: {:.2} GB", model_size_gb);

    // Try to get available GPU VRAM
    // For NVIDIA GPUs, we can estimate based on typical model requirements
    // A rough heuristic:
    // - Small models (< 5GB): Use all GPU layers (typically ~40 layers)
    // - Medium models (5-15GB): Use proportional layers
    // - Large models (> 15GB): May need CPU offload

    // Estimate based on RTX 4090 with ~24GB VRAM
    // Reserve ~2GB for system/context, leaving ~22GB for model
    let available_vram_gb = 22.0;

    println!("[GPU] Estimated available VRAM: {:.2} GB", available_vram_gb);

    // Calculate what percentage of the model fits in VRAM
    let vram_ratio = (available_vram_gb / model_size_gb).min(1.0);

    // Estimate typical layer count based on model size
    // Small models (~7B params, ~4-8GB): ~32-40 layers
    // Medium models (~13B params, ~8-15GB): ~40-50 layers
    // Large models (~30B+ params, >15GB): ~50-80 layers
    let estimated_total_layers = if model_size_gb < 8.0 {
        36
    } else if model_size_gb < 15.0 {
        45
    } else if model_size_gb < 25.0 {
        60
    } else {
        80
    };

    let optimal_layers = (estimated_total_layers as f64 * vram_ratio).floor() as u32;

    println!("[GPU] Estimated total layers: {}", estimated_total_layers);
    println!("[GPU] VRAM utilization ratio: {:.1}%", vram_ratio * 100.0);
    println!("[GPU] Optimal GPU layers: {} ({}% of model)",
             optimal_layers,
             (optimal_layers as f64 / estimated_total_layers as f64 * 100.0) as u32);

    // Ensure at least 1 layer on GPU if model is small enough
    optimal_layers.max(if vram_ratio > 0.1 { 1 } else { 0 })
}

// Helper function to load a model
#[cfg(feature = "docker")]
async fn load_model(llama_state: SharedLlamaState, model_path: &str) -> Result<(), String> {
    println!("[DEBUG] load_model called with path: {}", model_path);

    // Handle poisoned mutex by recovering from panic
    let mut state_guard = llama_state.lock().unwrap_or_else(|poisoned| {
        println!("[DEBUG] Mutex was poisoned, recovering...");
        poisoned.into_inner()
    });

    // Initialize backend if needed
    if state_guard.is_none() {
        let backend = LlamaBackend::init().map_err(|e| format!("Failed to init backend: {}", e))?;
        *state_guard = Some(LlamaState {
            backend,
            model: None,
            current_model_path: None,
            model_context_length: None,
            chat_template_type: None,
            last_used: std::time::SystemTime::now(),
        });
    }

    let state = state_guard.as_mut().unwrap();

    // Check if model is already loaded
    if let Some(ref current_path) = state.current_model_path {
        if current_path == model_path && state.model.is_some() {
            state.last_used = std::time::SystemTime::now();
            return Ok(()); // Model already loaded
        }
    }

    // Unload current model if any
    state.model = None;
    state.current_model_path = None;

    // Calculate optimal GPU layers
    let optimal_gpu_layers = calculate_optimal_gpu_layers(model_path);

    // Load new model with calculated GPU acceleration
    let model_params = LlamaModelParams::default()
        .with_n_gpu_layers(optimal_gpu_layers);

    println!("Loading model from: {}", model_path);
    println!("GPU layers configured: {} layers will be offloaded to GPU", optimal_gpu_layers);

    let model = LlamaModel::load_from_file(&state.backend, model_path, &model_params)
        .map_err(|e| format!("Failed to load model: {}", e))?;

    println!("Model loaded successfully!");

    // Read model's context length, token IDs, and chat template from GGUF metadata
    let (model_context_length, bos_token_id, eos_token_id, chat_template_type) = if let Ok(file) = fs::File::open(model_path) {
        let mut reader = BufReader::new(file);
        if let Ok(header) = GgufHeader::parse(&mut reader) {
            if let Ok(metadata) = GgufReader::read_metadata(&mut reader, header.n_kv) {
                let ctx_len = metadata.get("llama.context_length")
                    .and_then(|v| match v {
                        Value::Uint32(n) => Some(*n),
                        Value::Uint64(n) => Some(*n as u32),
                        _ => None,
                    });

                let bos_id = metadata.get("tokenizer.ggml.bos_token_id")
                    .and_then(|v| match v {
                        Value::Uint32(n) => Some(*n as i32),
                        Value::Int32(n) => Some(*n),
                        _ => None,
                    });

                let eos_id = metadata.get("tokenizer.ggml.eos_token_id")
                    .and_then(|v| match v {
                        Value::Uint32(n) => Some(*n as i32),
                        Value::Int32(n) => Some(*n),
                        _ => None,
                    });

                // Detect chat template type
                let template_type = metadata.get("tokenizer.chat_template")
                    .and_then(|v| match v {
                        Value::String(s) => {
                            // Detect template type based on template content
                            if s.contains("<|im_start|>") && s.contains("<|im_end|>") {
                                Some("ChatML".to_string()) // Qwen, OpenAI format
                            } else if s.contains("[INST]") && s.contains("[/INST]") {
                                Some("Mistral".to_string()) // Mistral format
                            } else if s.contains("<|start_header_id|>") {
                                Some("Llama3".to_string()) // Llama 3 format
                            } else {
                                Some("Generic".to_string()) // Fallback
                            }
                        }
                        _ => None,
                    });

                (ctx_len, bos_id, eos_id, template_type)
            } else {
                (None, None, None, None)
            }
        } else {
            (None, None, None, None)
        }
    } else {
        (None, None, None, None)
    };

    if let Some(ctx_len) = model_context_length {
        println!("Model context length from GGUF: {}", ctx_len);
    }
    if let Some(bos) = bos_token_id {
        println!("Model BOS token ID from GGUF: {}", bos);
    }
    if let Some(eos) = eos_token_id {
        println!("Model EOS token ID from GGUF: {}", eos);

        // Validate against what the model reports
        let model_eos = model.token_eos().0; // Extract underlying i32 from LlamaToken
        if eos != model_eos {
            println!("WARNING: GGUF EOS token ({}) doesn't match model.token_eos() ({})", eos, model_eos);
        } else {
            println!("✓ EOS token validation passed: GGUF and model agree on token {}", eos);
        }
    }

    if let Some(ref template) = chat_template_type {
        println!("Detected chat template type: {}", template);
    } else {
        println!("No chat template detected, using Mistral format as default");
    }

    state.model = Some(model);
    state.current_model_path = Some(model_path.to_string());
    state.model_context_length = model_context_length;
    state.chat_template_type = chat_template_type;
    state.last_used = std::time::SystemTime::now();

    Ok(())
}

// Helper function to unload the current model
#[cfg(feature = "docker")]
async fn unload_model(llama_state: SharedLlamaState) -> Result<(), String> {
    let mut state_guard = llama_state.lock().map_err(|_| "Failed to lock LLaMA state")?;
    
    if let Some(state) = state_guard.as_mut() {
        state.model = None;
        state.current_model_path = None;
    }
    
    Ok(())
}

// Conversation logging
struct ConversationLogger {
    file_path: String,
    content: String,
}

impl ConversationLogger {
    fn new(system_prompt: Option<&str>) -> io::Result<Self> {
        // Create assets/conversations directory if it doesn't exist
        let conversations_dir = "assets/conversations";
        fs::create_dir_all(conversations_dir)?;

        // Generate timestamp-based filename with YYYY-MM-DD-HH-mm-ss-SSS format
        let now = std::time::SystemTime::now();
        let since_epoch = now
            .duration_since(std::time::UNIX_EPOCH)
            .map_err(|e| io::Error::new(io::ErrorKind::Other, e))?;

        // Convert to a more readable format
        let secs = since_epoch.as_secs();
        let millis = since_epoch.subsec_millis();

        // Simple conversion (this won't be perfect timezone-wise, but it's readable)
        let days_since_epoch = secs / 86400;
        let remaining_secs = secs % 86400;
        let hours = remaining_secs / 3600;
        let remaining_secs = remaining_secs % 3600;
        let minutes = remaining_secs / 60;
        let seconds = remaining_secs % 60;

        // Approximate date calculation (starting from 1970-01-01)
        let year = 1970 + (days_since_epoch / 365);
        let day_of_year = days_since_epoch % 365;
        let month = std::cmp::min(12, (day_of_year / 30) + 1);
        let day = (day_of_year % 30) + 1;

        let timestamp = format!(
            "{:04}-{:02}-{:02}-{:02}-{:02}-{:02}-{:03}",
            year, month, day, hours, minutes, seconds, millis
        );

        let file_path = format!("{}/chat_{}.txt", conversations_dir, timestamp);

        let mut logger = ConversationLogger {
            file_path,
            content: String::new(),
        };

        // Only log system prompt if one is explicitly provided
        // If None, the model's chat template will use its built-in default
        if let Some(prompt) = system_prompt {
            logger.log_message("SYSTEM", prompt);
        }

        Ok(logger)
    }

    fn log_message(&mut self, role: &str, message: &str) {
        let log_entry = format!("{}:\n{}\n\n", role, message);
        self.content.push_str(&log_entry);

        // Write immediately to file
        if let Err(e) = fs::write(&self.file_path, &self.content) {
            eprintln!("Failed to write conversation log: {}", e);
        }
    }

    fn log_token(&mut self, token: &str) {
        // Append token to the last assistant message in content
        self.content.push_str(token);
        
        // Write immediately to file
        if let Err(e) = fs::write(&self.file_path, &self.content) {
            eprintln!("Failed to write conversation log: {}", e);
        }
    }
    
    fn finish_assistant_message(&mut self) {
        // Add proper newlines after assistant message completion
        self.content.push_str("\n\n");

        // Write immediately to file
        if let Err(e) = fs::write(&self.file_path, &self.content) {
            eprintln!("Failed to write conversation log: {}", e);
        }
    }

    fn log_command_execution(&mut self, command: &str, output: &str) {
        let log_entry = format!("[COMMAND: {}]\n{}\n\n", command, output);
        self.content.push_str(&log_entry);

        // Write immediately to file
        if let Err(e) = fs::write(&self.file_path, &self.content) {
            eprintln!("Failed to write conversation log: {}", e);
        }
    }

    fn get_full_conversation(&self) -> String {
        // Return the complete conversation content from memory
        self.content.clone()
    }

    fn load_conversation_from_file(&self) -> io::Result<String> {
        // Read the conversation directly from file (source of truth)
        fs::read_to_string(&self.file_path)
    }
}

fn parse_conversation_to_messages(conversation: &str) -> Vec<ChatMessage> {
    let mut messages = Vec::new();
    let mut current_role = "";
    let mut current_content = String::new();
    
    for line in conversation.lines() {
        if line.ends_with(":")
            && (line.starts_with("SYSTEM:")
                || line.starts_with("USER:")
                || line.starts_with("ASSISTANT:"))
        {
            // Save previous message if it exists
            if !current_role.is_empty() && !current_content.trim().is_empty() {
                let role = match current_role {
                    "SYSTEM" => "system",
                    "USER" => "user", 
                    "ASSISTANT" => "assistant",
                    _ => "user",
                };
                
                // Skip system messages in the UI
                if role != "system" {
                    messages.push(ChatMessage {
                        id: uuid::Uuid::new_v4().to_string(),
                        role: role.to_string(),
                        content: current_content.trim().to_string(),
                        timestamp: std::time::SystemTime::now()
                            .duration_since(std::time::UNIX_EPOCH)
                            .unwrap_or_default()
                            .as_secs(),
                    });
                }
            }
            
            // Start new message
            current_role = line.trim_end_matches(":");
            current_content.clear();
        } else if !line.starts_with("[COMMAND:") && !line.trim().is_empty() {
            // Skip command execution logs, add content
            current_content.push_str(line);
            current_content.push('\n');
        }
    }
    
    // Add the final message
    if !current_role.is_empty() && !current_content.trim().is_empty() {
        let role = match current_role {
            "SYSTEM" => "system",
            "USER" => "user",
            "ASSISTANT" => "assistant", 
            _ => "user",
        };
        
        // Skip system messages in the UI
        if role != "system" {
            messages.push(ChatMessage {
                id: uuid::Uuid::new_v4().to_string(),
                role: role.to_string(),
                content: current_content.trim().to_string(),
                timestamp: std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs(),
            });
        }
    }
    
    messages
}

#[cfg(feature = "docker")]
fn get_available_tools_json() -> String {
    // Detect OS and provide appropriate command examples
    let os_name = std::env::consts::OS;
    let (description, example_commands) = match os_name {
        "windows" => (
            "Execute shell commands on Windows. Use 'dir' to list files, 'type' to read files, 'cd' to change directory, and other Windows cmd.exe commands.",
            "dir E:\\repo, type file.txt, cd C:\\Users"
        ),
        "linux" => (
            "Execute shell commands on Linux. Use 'ls' to list files, 'cat' to read files, 'cd' to change directory, and other bash commands.",
            "ls /home, cat file.txt, pwd"
        ),
        "macos" => (
            "Execute shell commands on macOS. Use 'ls' to list files, 'cat' to read files, 'cd' to change directory, and other bash commands.",
            "ls /Users, cat file.txt, pwd"
        ),
        _ => (
            "Execute shell commands on the system. Use this to interact with the filesystem, run programs, and check system information.",
            "Use appropriate commands for your operating system"
        )
    };

    serde_json::json!([
        {
            "type": "function",
            "function": {
                "name": "bash",
                "description": format!("{} OS: {}", description, os_name),
                "parameters": {
                    "type": "object",
                    "properties": {
                        "command": {
                            "type": "string",
                            "description": format!("The shell command to execute. Examples: {}", example_commands)
                        }
                    },
                    "required": ["command"]
                }
            }
        }
    ]).to_string()
}

fn apply_model_chat_template(conversation: &str, template_type: Option<&str>) -> Result<String, String> {
    // Parse conversation into messages
    let mut system_message: Option<String> = None;
    let mut user_messages = Vec::new();
    let mut assistant_messages = Vec::new();
    let mut current_role = "";
    let mut current_content = String::new();

    for line in conversation.lines() {
        if line.ends_with(":")
            && (line.starts_with("SYSTEM:")
                || line.starts_with("USER:")
                || line.starts_with("ASSISTANT:"))
        {
            // Save previous role's content
            if !current_role.is_empty() && !current_content.trim().is_empty() {
                match current_role {
                    "SYSTEM" => system_message = Some(current_content.trim().to_string()),
                    "USER" => user_messages.push(current_content.trim().to_string()),
                    "ASSISTANT" => assistant_messages.push(current_content.trim().to_string()),
                    _ => {}
                }
            }

            // Start new role
            current_role = line.trim_end_matches(":");
            current_content.clear();
        } else if !line.starts_with("[COMMAND:") {
            // Skip command execution logs, add content
            if !line.trim().is_empty() {
                current_content.push_str(line);
                current_content.push('\n');
            }
        }
    }

    // Add the final role content
    if !current_role.is_empty() && !current_content.trim().is_empty() {
        match current_role {
            "SYSTEM" => system_message = Some(current_content.trim().to_string()),
            "USER" => user_messages.push(current_content.trim().to_string()),
            "ASSISTANT" => assistant_messages.push(current_content.trim().to_string()),
            _ => {}
        }
    }

    // Construct prompt based on detected template type
    let prompt = match template_type {
        Some("ChatML") => {
            // Qwen/ChatML format: <|im_start|>role\ncontent<|im_end|>
            let mut p = String::new();

            // Add system message
            if let Some(sys_msg) = system_message {
                p.push_str("<|im_start|>system\n");
                p.push_str(&sys_msg);
                p.push_str("<|im_end|>\n");
            }

            // Add conversation history
            let turn_count = user_messages.len().max(assistant_messages.len());
            for i in 0..turn_count {
                if i < user_messages.len() {
                    p.push_str("<|im_start|>user\n");
                    p.push_str(&user_messages[i]);
                    p.push_str("<|im_end|>\n");
                }
                if i < assistant_messages.len() {
                    p.push_str("<|im_start|>assistant\n");
                    p.push_str(&assistant_messages[i]);
                    p.push_str("<|im_end|>\n");
                }
            }

            // Add generation prompt
            p.push_str("<|im_start|>assistant\n");

            p
        }
        Some("Mistral") | None => {
            // Mistral format: <s>[INST] user [/INST] assistant </s>
            let mut p = String::new();
            p.push_str("<s>");

            // Add system prompt if present
            if let Some(sys_msg) = system_message {
                p.push_str("[SYSTEM_PROMPT]");
                p.push_str(&sys_msg);
                p.push_str("[/SYSTEM_PROMPT]");
            }

            // Add conversation history
            let turn_count = user_messages.len().max(assistant_messages.len());
            for i in 0..turn_count {
                if i < user_messages.len() {
                    p.push_str("[INST]");
                    p.push_str(&user_messages[i]);
                    p.push_str("[/INST]");
                }
                if i < assistant_messages.len() {
                    p.push_str(&assistant_messages[i]);
                    p.push_str("</s>");
                }
            }

            p
        }
        Some("Llama3") => {
            // Llama 3 format
            let mut p = String::new();
            p.push_str("<|begin_of_text|>");

            if let Some(sys_msg) = system_message {
                p.push_str("<|start_header_id|>system<|end_header_id|>\n\n");
                p.push_str(&sys_msg);
                p.push_str("<|eot_id|>");
            }

            let turn_count = user_messages.len().max(assistant_messages.len());
            for i in 0..turn_count {
                if i < user_messages.len() {
                    p.push_str("<|start_header_id|>user<|end_header_id|>\n\n");
                    p.push_str(&user_messages[i]);
                    p.push_str("<|eot_id|>");
                }
                if i < assistant_messages.len() {
                    p.push_str("<|start_header_id|>assistant<|end_header_id|>\n\n");
                    p.push_str(&assistant_messages[i]);
                    p.push_str("<|eot_id|>");
                }
            }

            p.push_str("<|start_header_id|>assistant<|end_header_id|>\n\n");

            p
        }
        Some(_) => {
            // Generic fallback - use ChatML-style
            let mut p = String::new();

            if let Some(sys_msg) = system_message {
                p.push_str("System: ");
                p.push_str(&sys_msg);
                p.push_str("\n\n");
            }

            let turn_count = user_messages.len().max(assistant_messages.len());
            for i in 0..turn_count {
                if i < user_messages.len() {
                    p.push_str("User: ");
                    p.push_str(&user_messages[i]);
                    p.push_str("\n\n");
                }
                if i < assistant_messages.len() {
                    p.push_str("Assistant: ");
                    p.push_str(&assistant_messages[i]);
                    p.push_str("\n\n");
                }
            }

            p.push_str("Assistant: ");

            p
        }
    };

    // Debug: Print first 1000 chars of prompt
    eprintln!("\n[DEBUG] Template type: {:?}", template_type);
    eprintln!("[DEBUG] Constructed prompt (first 1000 chars):");
    eprintln!("{}", &prompt.chars().take(1000).collect::<String>());

    Ok(prompt)
}

// Constants for LLaMA configuration
#[cfg(feature = "docker")]
const CONTEXT_SIZE: u32 = 32768;
#[cfg(feature = "docker")]
const MODEL_PATH: &str = "/app/models/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf";

#[cfg(feature = "docker")]
async fn generate_llama_response(
    user_message: &str,
    llama_state: SharedLlamaState,
    conversation_logger: SharedConversationLogger,
    token_sender: Option<mpsc::UnboundedSender<TokenData>>
) -> Result<(String, i32, i32), String> {
    // Log user message to conversation file
    {
        let mut logger = conversation_logger.lock().map_err(|_| "Failed to lock conversation logger")?;
        logger.log_message("USER", user_message);
    }
    
    // Load configuration to get model path and context size
    let config = load_config();
    let model_path = config.model_path.as_deref().unwrap_or(MODEL_PATH);
    let stop_tokens = config.stop_tokens.unwrap_or_else(get_common_stop_tokens);
    
    // Ensure model is loaded
    load_model(llama_state.clone(), model_path).await?;

    // Now use the shared state for generation
    let state_guard = llama_state.lock().map_err(|_| "Failed to lock LLaMA state")?;
    let state = state_guard.as_ref().ok_or("LLaMA state not initialized")?;
    let model = state.model.as_ref().ok_or("No model loaded")?;

    // Get context size: prefer user config, fallback to model's context_length, then default
    let context_size = config.context_size
        .or(state.model_context_length)
        .unwrap_or(CONTEXT_SIZE);

    println!("Using context size: {} (user config: {:?}, model max: {:?})",
        context_size, config.context_size, state.model_context_length);
    
    // Create sampler
    let mut sampler = LlamaSampler::greedy();
    
    // Read conversation history from file and create chat prompt
    let conversation_content = {
        let logger = conversation_logger.lock().map_err(|_| "Failed to lock conversation logger")?;
        logger.load_conversation_from_file().unwrap_or_else(|_| logger.get_full_conversation())
    };

    // Convert conversation to chat format using model's chat template
    let template_type = state.chat_template_type.clone();
    let prompt = apply_model_chat_template(&conversation_content, template_type.as_deref())?;
    
    // Tokenize
    let tokens = model
        .str_to_token(&prompt, AddBos::Never)
        .map_err(|e| format!("Tokenization failed: {}", e))?;
    
    // Create context with configured size
    let n_ctx = NonZeroU32::new(context_size).unwrap();
    let ctx_params = LlamaContextParams::default().with_n_ctx(Some(n_ctx));
    let mut context = model
        .new_context(&state.backend, ctx_params)
        .map_err(|e| format!("Context creation failed: {}", e))?;
    
    // Prepare batch
    let batch_size = std::cmp::min(tokens.len() + 512, 2048);
    let mut batch = LlamaBatch::new(batch_size, 1);
    for (i, &token) in tokens.iter().enumerate() {
        let is_last = i == tokens.len() - 1;
        batch
            .add(token, i as i32, &[0], is_last)
            .map_err(|e| format!("Batch add failed: {}", e))?;
    }
    
    // Process initial tokens
    context
        .decode(&mut batch)
        .map_err(|e| format!("Initial decode failed: {}", e))?;
    
    // Start assistant message in conversation log
    {
        let mut logger = conversation_logger.lock().map_err(|_| "Failed to lock conversation logger")?;
        logger.log_message("ASSISTANT", "");
    }

    // Generate response
    let mut response = String::new();
    let mut token_pos = tokens.len() as i32;
    let mut total_tokens_generated = 0;

    // Calculate max tokens based on remaining context space
    // Leave some buffer for safety (e.g., 128 tokens)
    let remaining_context = (context_size as i32) - token_pos - 128;
    let max_total_tokens = remaining_context.max(512); // Ensure at least 512 tokens if possible

    println!("Context size: {}, Prompt tokens: {}, Max tokens to generate: {}",
             context_size, token_pos, max_total_tokens);

    // Outer loop to handle command execution and continuation
    loop {
        let mut command_executed = false;
        let mut hit_stop_condition = false; // Track if we hit EOS or stop token

        // Inner loop for token generation
        let tokens_to_generate = std::cmp::min(2048, max_total_tokens - total_tokens_generated);

        println!("[DEBUG] Starting generation cycle: tokens_to_generate={}, total_tokens_generated={}", tokens_to_generate, total_tokens_generated);

        for _i in 0..tokens_to_generate { // Limit response length per cycle
        // Sample next token
        let next_token = sampler.sample(&context, -1);

        // Check for end-of-sequence token
        if next_token == model.token_eos() {
            println!("Debug: Stopping generation - EOS token detected (token ID: {})", next_token);
            hit_stop_condition = true;
            break;
        }

        // IMPORTANT: Add token to batch and decode FIRST, before string conversion
        // This ensures the model progresses even if we can't display the token
        batch.clear();
        batch
            .add(next_token, token_pos, &[0], true)
            .map_err(|e| format!("Batch add failed: {}", e))?;

        context
            .decode(&mut batch)
            .map_err(|e| format!("Decode failed: {}", e))?;

        token_pos += 1;
        total_tokens_generated += 1;

        // Now try to convert to string for display - if this fails, we skip display but keep going
        let token_str = match model.token_to_str(next_token, Special::Tokenize) {
            Ok(s) => s,
            Err(e) => {
                // Log UTF-8 error but continue generation
                // Token is already processed in context, just can't display it
                println!("[WARN] Token {} can't be displayed as UTF-8: {}. Continuing generation.", next_token, e);
                continue; // Skip display but token is already in context
            }
        };

        // Debug: log suspicious tokens
        if token_str.contains('<') || token_str.contains('>') || token_str.contains('|') {
            println!("Debug: Found special character in token: '{}'", token_str);
        }

        // IMPORTANT: Check for stop sequences BEFORE adding the token to the response
        // This prevents the stop sequences from appearing in the final output
        let test_response = format!("{}{}", response, token_str);

        // Check for any configured stop tokens
        let mut should_stop = false;
        let mut partial_to_remove = 0;

        // Check if we're inside a COMMAND block
        let in_command_block = response.contains("<COMMAND>") && !response.contains("</COMMAND>");

        for stop_token in &stop_tokens {
            // Check if the test response contains the complete stop token
            if test_response.contains(stop_token) {
                println!("Debug: Stopping generation due to stop token detected: '{}'", stop_token);

                // Special case: for </COMMAND>, we want to include it in the response
                if stop_token == "</COMMAND>" {
                    // Add the current token to complete the </COMMAND> tag
                    response.push_str(&token_str);

                    // Log token to conversation file
                    {
                        let mut logger = conversation_logger.lock().map_err(|_| "Failed to lock conversation logger")?;
                        logger.log_token(&token_str);
                    }
                }

                should_stop = true;
                break;
            }

            // Special handling when inside a COMMAND block
            // Don't stop on partial matches of stop tokens that start with "</" while generating a command
            if in_command_block && (stop_token.starts_with("</") || stop_token.starts_with("[/")) {
                continue; // Skip these stop tokens while inside COMMAND block
            }

            // Special handling for </COMMAND> - only stop on complete match, no partial matching
            // This allows the full command to be generated before stopping
            if stop_token == "</COMMAND>" {
                continue; // Skip partial matching for </COMMAND>
            }

            // Check if we're at the beginning of a stop token (partial match at the end)
            // This prevents incomplete stop tokens from appearing
            // Only check for partial matches of 2+ characters to avoid false positives on single '<' or '['
            if stop_token.len() > 2 {
                let trimmed = test_response.trim_end();
                for i in 2..stop_token.len() {
                    if trimmed.ends_with(&stop_token[..i]) {
                        println!("Debug: Stopping generation due to partial stop token: '{}' (partial: '{}')",
                                 stop_token, &stop_token[..i]);

                        // Check if the partial exists in the current response (before adding new token)
                        // If so, we need to remove it
                        if response.trim_end().ends_with(&stop_token[..i-token_str.len()]) && i > token_str.len() {
                            partial_to_remove = i - token_str.len();
                        }

                        should_stop = true;
                        break;
                    }
                }
                if should_stop {
                    break;
                }
            }
        }

        if should_stop {
            // Remove any partial stop token from the response
            if partial_to_remove > 0 {
                let new_len = response.len().saturating_sub(partial_to_remove);
                response.truncate(new_len);
                println!("Debug: Removed {} characters of partial stop token from response", partial_to_remove);
            }
            hit_stop_condition = true;
            break;
        }

        // If no stop sequence detected, add the token to the response
        response.push_str(&token_str);

        // Send token through channel for streaming (if enabled)
        if let Some(ref sender) = token_sender {
            println!("[STREAMING] Sending token: '{}'", token_str);
            let token_data = TokenData {
                token: token_str.clone(),
                tokens_used: token_pos,
                max_tokens: context_size as i32,
            };
            let _ = sender.send(token_data);
        }

        // Log token to conversation file
        {
            let mut logger = conversation_logger.lock().map_err(|_| "Failed to lock conversation logger")?;
            logger.log_token(&token_str);
        }
    } // End of inner token generation loop

        // Check if response contains a command to execute
        if response.contains("<COMMAND>") && response.contains("</COMMAND>") {
            if let Some(start) = response.find("<COMMAND>") {
                if let Some(end) = response.find("</COMMAND>") {
                    if end > start {
                        let command_text = &response[start + 9..end]; // 9 is length of "<COMMAND>"
                        println!("Debug: Executing command: {}", command_text);

                        // Execute the command
                        let output = execute_command(command_text);
                        println!("Debug: Command output: {}", output);

                        // Log command execution
                        {
                            let mut logger = conversation_logger.lock().map_err(|_| "Failed to lock conversation logger")?;
                            logger.log_command_execution(command_text, &output);
                        }

                        // Replace the command in the response with output
                        let before_command = &response[..start];
                        let after_command = &response[end + 10..]; // 10 is length of "</COMMAND>"

                        let command_output_text = format!(
                            "\n\n[COMMAND: {}]\n\n```\n{}\n```\n\n",
                            command_text,
                            output.trim()
                        );

                        response = format!(
                            "{}{}{}",
                            before_command.trim(),
                            command_output_text,
                            after_command
                        );

                        // Log the command output to conversation
                        {
                            let mut logger = conversation_logger.lock().map_err(|_| "Failed to lock conversation logger")?;
                            logger.log_token(&command_output_text);
                        }

                        // Tokenize the command output and feed it to the context
                        let output_tokens = model
                            .str_to_token(&command_output_text, AddBos::Never)
                            .map_err(|e| format!("Tokenization of command output failed: {}", e))?;

                        println!("Debug: Feeding {} command output tokens to context", output_tokens.len());

                        // Feed output tokens to context
                        for token in output_tokens {
                            batch.clear();
                            batch
                                .add(token, token_pos, &[0], true)
                                .map_err(|e| format!("Batch add failed for command output: {}", e))?;

                            context
                                .decode(&mut batch)
                                .map_err(|e| format!("Decode failed for command output: {}", e))?;

                            token_pos += 1;
                        }

                        command_executed = true;
                    }
                }
            }
        }

        // Break if we hit a stop condition (EOS/stop token) or reached token limit
        // Continue if we just completed a generation cycle without hitting stop
        if hit_stop_condition || total_tokens_generated >= max_total_tokens {
            println!("[DEBUG] Exiting outer loop: hit_stop_condition={}, total_tokens_generated={}, max_total_tokens={}",
                     hit_stop_condition, total_tokens_generated, max_total_tokens);
            break;
        }

        // Only continue outer loop if command was executed (for command execution workflow)
        if !command_executed {
            // No command executed and no stop condition - continue generating
            println!("[DEBUG] Continuing generation: no stop condition hit");
        }
    } // End of outer command execution loop

    // Finish the assistant message with proper formatting
    {
        let mut logger = conversation_logger.lock().map_err(|_| "Failed to lock conversation logger")?;
        logger.finish_assistant_message();
    }

    Ok((response.trim().to_string(), token_pos, max_total_tokens))
}

#[cfg(feature = "docker")]
async fn handle_request(
    req: Request<Body>,
    llama_state: SharedLlamaState,
) -> std::result::Result<Response<Body>, Infallible> {
    handle_request_impl(req, Some(llama_state)).await
}

#[cfg(not(feature = "docker"))]
async fn handle_request(
    req: Request<Body>,
) -> std::result::Result<Response<Body>, Infallible> {
    handle_request_impl(req, None).await
}

async fn handle_request_impl(
    req: Request<Body>,
    #[cfg(feature = "docker")]
    llama_state: Option<SharedLlamaState>,
    #[cfg(not(feature = "docker"))]
    _llama_state: Option<()>,
) -> std::result::Result<Response<Body>, Infallible> {
    let response = match (req.method(), req.uri().path()) {
        (&Method::GET, "/health") => {
            Response::builder()
                .status(StatusCode::OK)
                .header("content-type", "application/json")
                .header("access-control-allow-origin", "*")
                .body(Body::from(r#"{"status":"ok","service":"llama-chat-web"}"#))
                .unwrap()
        }
        
        (&Method::POST, "/api/chat") => {
            // Parse request body
            let body_bytes = match hyper::body::to_bytes(req.into_body()).await {
                Ok(bytes) => bytes,
                Err(_) => {
                    return Ok(Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Failed to read request body"}"#))
                        .unwrap());
                }
            };

            // Debug: log the received JSON
            if let Ok(body_str) = std::str::from_utf8(&body_bytes) {
                println!("Received request body: {}", body_str);
            }
            
            let chat_request: ChatRequest = match serde_json::from_slice(&body_bytes) {
                Ok(req) => req,
                Err(e) => {
                    println!("JSON parsing error: {}", e);
                    return Ok(Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Invalid JSON format"}"#))
                        .unwrap());
                }
            };

            #[cfg(feature = "docker")]
            {
                // Check for test mode environment variable
                if std::env::var("TEST_MODE").unwrap_or_default() == "true" {
                    // Fast test response
                    let test_response = format!(
                        "Hello! This is a test response to your message: '{}'", 
                        chat_request.message
                    );
                    
                    let response = ChatResponse {
                        message: ChatMessage {
                            id: format!("{}", uuid::Uuid::new_v4()),
                            role: "assistant".to_string(),
                            content: test_response,
                            timestamp: std::time::SystemTime::now()
                                .duration_since(std::time::UNIX_EPOCH)
                                .unwrap_or_default()
                                .as_secs(),
                        },
                        conversation_id: chat_request.conversation_id.unwrap_or_else(|| format!("{}", uuid::Uuid::new_v4())),
                        tokens_used: None,
                        max_tokens: None,
                    };
                    
                    return Ok(Response::builder()
                        .status(StatusCode::OK)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(serde_json::to_string(&response).unwrap()))
                        .unwrap());
                }
                
                // Load configuration to get system prompt
                let config = load_config();
                let system_prompt = config.system_prompt.as_deref();
                
                // Create a new conversation logger for this chat session
                let conversation_logger = match ConversationLogger::new(system_prompt) {
                    Ok(logger) => Arc::new(Mutex::new(logger)),
                    Err(e) => {
                        return Ok(Response::builder()
                            .status(StatusCode::INTERNAL_SERVER_ERROR)
                            .header("content-type", "application/json")
                            .header("access-control-allow-origin", "*")
                            .body(Body::from(format!(r#"{{"error":"Failed to create conversation logger: {}"}}"#, e)))
                            .unwrap());
                    }
                };
                
                // Generate actual LLaMA response
                let (response_content, tokens_used, max_tokens) = match llama_state {
                    Some(state) => {
                        match generate_llama_response(&chat_request.message, state, conversation_logger, None).await {
                            Ok((content, tokens, max_tok)) => (content, Some(tokens), Some(max_tok)),
                            Err(err) => (format!("Error generating response: {}", err), None, None),
                        }
                    }
                    None => ("LLaMA state not available".to_string(), None, None),
                };

                let chat_response = ChatResponse {
                    message: ChatMessage {
                        id: uuid::Uuid::new_v4().to_string(),
                        role: "assistant".to_string(),
                        content: response_content,
                        timestamp: std::time::SystemTime::now()
                            .duration_since(std::time::UNIX_EPOCH)
                            .unwrap_or_default()
                            .as_secs(),
                    },
                    conversation_id: chat_request.conversation_id
                        .unwrap_or_else(|| uuid::Uuid::new_v4().to_string()),
                    tokens_used,
                    max_tokens,
                };

                let response_json = match serde_json::to_string(&chat_response) {
                    Ok(json) => json,
                    Err(_) => r#"{"error":"Failed to serialize response"}"#.to_string(),
                };

                Response::builder()
                    .status(StatusCode::OK)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .header("access-control-allow-methods", "GET, POST, OPTIONS")
                    .header("access-control-allow-headers", "content-type")
                    .body(Body::from(response_json))
                    .unwrap()
            }

            #[cfg(not(feature = "docker"))]
            {
                // Fallback mock response when not using docker feature
                let mock_response = r#"{"message":{"id":"test","role":"assistant","content":"LLaMA integration not available (docker feature not enabled)","timestamp":1234567890},"conversation_id":"test-conversation"}"#;
                Response::builder()
                    .status(StatusCode::OK)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .header("access-control-allow-methods", "GET, POST, OPTIONS")
                    .header("access-control-allow-headers", "content-type")
                    .body(Body::from(mock_response))
                    .unwrap()
            }
        }

        (&Method::POST, "/api/chat/stream") => {
            println!("[STREAMING] Received request to /api/chat/stream");
            // Parse request body
            let body_bytes = match hyper::body::to_bytes(req.into_body()).await {
                Ok(bytes) => bytes,
                Err(_) => {
                    return Ok(Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Failed to read request body"}"#))
                        .unwrap());
                }
            };

            let chat_request: ChatRequest = match serde_json::from_slice(&body_bytes) {
                Ok(req) => req,
                Err(e) => {
                    println!("JSON parsing error: {}", e);
                    return Ok(Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Invalid JSON format"}"#))
                        .unwrap());
                }
            };

            #[cfg(feature = "docker")]
            {
                // Load configuration to get system prompt
                let config = load_config();
                let system_prompt = config.system_prompt.as_deref();

                // Create a new conversation logger for this chat session
                let conversation_logger = match ConversationLogger::new(system_prompt) {
                    Ok(logger) => Arc::new(Mutex::new(logger)),
                    Err(e) => {
                        return Ok(Response::builder()
                            .status(StatusCode::INTERNAL_SERVER_ERROR)
                            .header("content-type", "application/json")
                            .header("access-control-allow-origin", "*")
                            .body(Body::from(format!(r#"{{"error":"Failed to create conversation logger: {}"}}"#, e)))
                            .unwrap());
                    }
                };

                // Create channel for streaming tokens
                let (tx, mut rx) = mpsc::unbounded_channel::<TokenData>();

                // Spawn generation task
                let message = chat_request.message.clone();
                let state_clone = llama_state.clone();
                tokio::spawn(async move {
                    if let Some(state) = state_clone {
                        match generate_llama_response(&message, state, conversation_logger, Some(tx)).await {
                            Ok((content, tokens, max)) => {
                                println!("[DEBUG] Generation completed successfully: {} tokens used, {} max", tokens, max);
                            }
                            Err(e) => {
                                println!("[ERROR] Generation failed: {}", e);
                            }
                        }
                    } else {
                        println!("[ERROR] No LLaMA state available for generation");
                    }
                });

                // Stream tokens as Server-Sent Events
                // Send TokenData objects with metadata for real-time token count updates
                let stream = async_stream::stream! {
                    loop {
                        match rx.recv().await {
                            Some(token_data) => {
                                // Send TokenData as JSON
                                let json = serde_json::to_string(&token_data).unwrap_or_else(|_| r#"{"token":"","tokens_used":0,"max_tokens":0}"#.to_string());
                                let event = format!("data: {}\n\n", json);
                                yield Ok::<_, Infallible>(Bytes::from(event));
                            }
                            None => {
                                // Channel closed, break
                                break;
                            }
                        }
                    }
                    // Send done event
                    yield Ok::<_, Infallible>(Bytes::from("data: [DONE]\n\n"));
                };

                Response::builder()
                    .status(StatusCode::OK)
                    .header("content-type", "text/event-stream")
                    .header("cache-control", "no-cache")
                    .header("access-control-allow-origin", "*")
                    .header("connection", "keep-alive")
                    .header("x-accel-buffering", "no")  // Disable nginx buffering
                    .body(Body::wrap_stream(stream))
                    .unwrap()
            }

            #[cfg(not(feature = "docker"))]
            {
                Response::builder()
                    .status(StatusCode::OK)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"error":"Streaming not available (docker feature not enabled)"}"#))
                    .unwrap()
            }
        }

        (&Method::GET, "/api/config") => {
            // Load current configuration from file or use defaults
            let config = load_config();
            
            match serde_json::to_string(&config) {
                Ok(config_json) => {
                    Response::builder()
                        .status(StatusCode::OK)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(config_json))
                        .unwrap()
                }
                Err(_) => {
                    Response::builder()
                        .status(StatusCode::INTERNAL_SERVER_ERROR)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Failed to serialize configuration"}"#))
                        .unwrap()
                }
            }
        }
        
        (&Method::POST, "/api/config") => {
            // Parse request body for configuration update
            let body_bytes = match hyper::body::to_bytes(req.into_body()).await {
                Ok(bytes) => bytes,
                Err(_) => {
                    return Ok(Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Failed to read request body"}"#))
                        .unwrap());
                }
            };

            // Parse incoming config
            let incoming_config: SamplerConfig = match serde_json::from_slice(&body_bytes) {
                Ok(config) => config,
                Err(_) => {
                    return Ok(Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Invalid JSON format"}"#))
                        .unwrap());
                }
            };

            // Load existing config to preserve model_history
            let mut existing_config = load_config();

            // Update fields from incoming config, but preserve model_history
            existing_config.sampler_type = incoming_config.sampler_type;
            existing_config.temperature = incoming_config.temperature;
            existing_config.top_p = incoming_config.top_p;
            existing_config.top_k = incoming_config.top_k;
            existing_config.mirostat_tau = incoming_config.mirostat_tau;
            existing_config.mirostat_eta = incoming_config.mirostat_eta;
            existing_config.model_path = incoming_config.model_path;
            existing_config.system_prompt = incoming_config.system_prompt;
            existing_config.context_size = incoming_config.context_size;
            existing_config.stop_tokens = incoming_config.stop_tokens;
            // Note: model_history is NOT updated from incoming config

            // Save merged configuration to file
            let config_path = "assets/config.json";
            if let Err(_) = fs::create_dir_all("assets") {
                return Ok(Response::builder()
                    .status(StatusCode::INTERNAL_SERVER_ERROR)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"error":"Failed to create config directory"}"#))
                    .unwrap());
            }

            match fs::write(config_path, serde_json::to_string_pretty(&existing_config).unwrap_or_default()) {
                Ok(_) => {
                    Response::builder()
                        .status(StatusCode::OK)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"success":true}"#))
                        .unwrap()
                }
                Err(_) => {
                    Response::builder()
                        .status(StatusCode::INTERNAL_SERVER_ERROR)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Failed to save configuration"}"#))
                        .unwrap()
                }
            }
        }
        
        (&Method::GET, path) if path.starts_with("/api/conversation/") => {
            // Extract filename from path: /api/conversation/{filename}
            let filename = &path[18..]; // Remove "/api/conversation/"
            let conversations_dir = "assets/conversations";
            let file_path = format!("{}/{}", conversations_dir, filename);
            
            match fs::read_to_string(&file_path) {
                Ok(content) => {
                    let messages = parse_conversation_to_messages(&content);
                    let response = ConversationContentResponse {
                        content: content.clone(),
                        messages,
                    };
                    
                    let response_json = match serde_json::to_string(&response) {
                        Ok(json) => json,
                        Err(_) => r#"{"content":"","messages":[]}"#.to_string(),
                    };
                    
                    Response::builder()
                        .status(StatusCode::OK)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(response_json))
                        .unwrap()
                }
                Err(_) => {
                    Response::builder()
                        .status(StatusCode::NOT_FOUND)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Conversation not found"}"#))
                        .unwrap()
                }
            }
        }
        
        (&Method::GET, "/api/model/info") => {
            println!("[DEBUG] /api/model/info endpoint hit");

            // Extract model path from query parameters
            let query = req.uri().query().unwrap_or("");
            println!("[DEBUG] Query string: {}", query);

            let mut model_path = "";

            for param in query.split('&') {
                if let Some((key, value)) = param.split_once('=') {
                    if key == "path" {
                        // URL decode the path
                        model_path = value;
                        println!("[DEBUG] Found path parameter (encoded): {}", model_path);
                        break;
                    }
                }
            }

            if model_path.is_empty() {
                println!("[DEBUG] ERROR: No path parameter provided");
                return Ok(Response::builder()
                    .status(StatusCode::BAD_REQUEST)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"error":"Model path is required"}"#))
                    .unwrap());
            }

            // URL decode the path properly
            let decoded_path = urlencoding::decode(model_path).unwrap_or(std::borrow::Cow::Borrowed(model_path));
            println!("[DEBUG] Decoded path: {}", decoded_path);

            // Check if file exists
            let path_obj = std::path::Path::new(&*decoded_path);
            let exists = path_obj.exists();
            println!("[DEBUG] File exists: {}", exists);
            println!("[DEBUG] Path is file: {}", path_obj.is_file());
            println!("[DEBUG] Path is dir: {}", path_obj.is_dir());

            if !exists {
                println!("[DEBUG] ERROR: File does not exist at path: {}", decoded_path);
                return Ok(Response::builder()
                    .status(StatusCode::NOT_FOUND)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"error":"Model file not found"}"#))
                    .unwrap());
            }

            // Check if path is a directory
            if path_obj.is_dir() {
                println!("[DEBUG] Path is a directory, scanning for .gguf files...");

                // Find all .gguf files in the directory
                let mut gguf_files = Vec::new();
                if let Ok(entries) = fs::read_dir(path_obj) {
                    for entry in entries.flatten() {
                        let entry_path = entry.path();
                        if entry_path.is_file() {
                            if let Some(ext) = entry_path.extension() {
                                if ext.eq_ignore_ascii_case("gguf") {
                                    if let Some(filename) = entry_path.file_name().and_then(|n| n.to_str()) {
                                        gguf_files.push(filename.to_string());
                                    }
                                }
                            }
                        }
                    }
                }

                let response_json = if gguf_files.is_empty() {
                    serde_json::json!({
                        "error": "This is a directory. No .gguf files found in this directory.",
                        "is_directory": true,
                        "suggestions": []
                    })
                } else {
                    serde_json::json!({
                        "error": format!("This is a directory. Found {} .gguf file(s). Please select one:", gguf_files.len()),
                        "is_directory": true,
                        "suggestions": gguf_files
                    })
                };

                println!("[DEBUG] Returning directory error with {} suggestions", gguf_files.len());
                return Ok(Response::builder()
                    .status(StatusCode::BAD_REQUEST)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(response_json.to_string()))
                    .unwrap());
            }

            // Check if file has .gguf extension
            if let Some(ext) = path_obj.extension() {
                if !ext.eq_ignore_ascii_case("gguf") {
                    println!("[DEBUG] ERROR: File is not a .gguf file");
                    return Ok(Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"File must have .gguf extension"}"#))
                        .unwrap());
                }
            } else {
                println!("[DEBUG] ERROR: File has no extension");
                return Ok(Response::builder()
                    .status(StatusCode::BAD_REQUEST)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"error":"File must have .gguf extension"}"#))
                    .unwrap());
            }
            
            // Extract basic model information
            let file_metadata = match fs::metadata(&*decoded_path) {
                Ok(metadata) => metadata,
                Err(_) => {
                    return Ok(Response::builder()
                        .status(StatusCode::INTERNAL_SERVER_ERROR)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Failed to read file metadata"}"#))
                        .unwrap());
                }
            };
            
            let file_size_bytes = file_metadata.len();
            let file_size = if file_size_bytes >= 1_073_741_824 {
                format!("{:.1} GB", file_size_bytes as f64 / 1_073_741_824.0)
            } else if file_size_bytes >= 1_048_576 {
                format!("{:.1} MB", file_size_bytes as f64 / 1_048_576.0)
            } else {
                format!("{} bytes", file_size_bytes)
            };
            
            let filename = std::path::Path::new(&*decoded_path)
                .file_name()
                .and_then(|n| n.to_str())
                .unwrap_or("unknown");
            
            // Try to extract model information from filename patterns
            let mut architecture = "Unknown";
            let mut parameters = "Unknown";
            let mut quantization = "Unknown";
            
            // Common GGUF naming patterns
            if filename.contains("llama") || filename.contains("Llama") {
                architecture = "LLaMA";
            } else if filename.contains("mistral") || filename.contains("Mistral") {
                architecture = "Mistral";
            } else if filename.contains("qwen") || filename.contains("Qwen") {
                architecture = "Qwen";
            } else if filename.contains("phi") || filename.contains("Phi") {
                architecture = "Phi";
            }
            
            // Extract parameter count
            if filename.contains("7b") || filename.contains("7B") {
                parameters = "7B";
            } else if filename.contains("13b") || filename.contains("13B") {
                parameters = "13B";
            } else if filename.contains("70b") || filename.contains("70B") {
                parameters = "70B";
            } else if filename.contains("1.5b") || filename.contains("1.5B") {
                parameters = "1.5B";
            } else if filename.contains("3b") || filename.contains("3B") {
                parameters = "3B";
            }
            
            // Extract quantization
            if filename.contains("q4_0") || filename.contains("Q4_0") {
                quantization = "Q4_0";
            } else if filename.contains("q4_1") || filename.contains("Q4_1") {
                quantization = "Q4_1";
            } else if filename.contains("q5_0") || filename.contains("Q5_0") {
                quantization = "Q5_0";
            } else if filename.contains("q5_1") || filename.contains("Q5_1") {
                quantization = "Q5_1";
            } else if filename.contains("q8_0") || filename.contains("Q8_0") {
                quantization = "Q8_0";
            } else if filename.contains("f16") || filename.contains("F16") {
                quantization = "F16";
            } else if filename.contains("f32") || filename.contains("F32") {
                quantization = "F32";
            }
            
            // Estimate total layers based on model size
            let model_size_gb = file_size_bytes as f64 / (1024.0 * 1024.0 * 1024.0);
            let estimated_total_layers = if model_size_gb < 8.0 {
                36  // Small models (7B and below)
            } else if model_size_gb < 15.0 {
                45  // Medium models (13B)
            } else if model_size_gb < 25.0 {
                60  // Large models (30B)
            } else {
                80  // Very large models (70B+)
            };

            // Build base model info
            let mut model_info = serde_json::json!({
                "name": filename,
                "architecture": architecture,
                "parameters": parameters,
                "quantization": quantization,
                "file_size": file_size,
                "context_length": "Variable",
                "path": decoded_path.to_string(),
                "estimated_layers": estimated_total_layers
            });

            // Try to parse GGUF metadata
            if let Ok(file) = fs::File::open(&*decoded_path) {
                let mut reader = BufReader::new(file);

                if let Ok(header) = GgufHeader::parse(&mut reader) {
                    if let Ok(metadata) = GgufReader::read_metadata(&mut reader, header.n_kv) {
                        // Debug: Print all available metadata keys and values
                        println!("=== GGUF Metadata Found ===");
                        for (key, value) in metadata.iter() {
                            let val_str = match value {
                                Value::String(s) => format!("\"{}\"", s),
                                Value::Uint8(n) => n.to_string(),
                                Value::Uint16(n) => n.to_string(),
                                Value::Uint32(n) => n.to_string(),
                                Value::Uint64(n) => n.to_string(),
                                Value::Int8(n) => n.to_string(),
                                Value::Int16(n) => n.to_string(),
                                Value::Int32(n) => n.to_string(),
                                Value::Int64(n) => n.to_string(),
                                Value::Float32(f) => f.to_string(),
                                Value::Float64(f) => f.to_string(),
                                Value::Bool(b) => b.to_string(),
                                Value::Array(_, items) => format!("[Array with {} items]", items.len()),
                            };
                            println!("  {} = {}", key, val_str);
                        }
                        println!("================================");

                        // Helper to get metadata value as string
                        let get_meta_string = |key: &str| -> Option<String> {
                            metadata.get(key).and_then(|v| match v {
                                Value::String(s) => Some(s.clone()),
                                Value::Uint8(n) => Some(n.to_string()),
                                Value::Uint16(n) => Some(n.to_string()),
                                Value::Uint32(n) => Some(n.to_string()),
                                Value::Uint64(n) => Some(n.to_string()),
                                Value::Int8(n) => Some(n.to_string()),
                                Value::Int16(n) => Some(n.to_string()),
                                Value::Int32(n) => Some(n.to_string()),
                                Value::Int64(n) => Some(n.to_string()),
                                Value::Float32(f) => Some(f.to_string()),
                                Value::Float64(f) => Some(f.to_string()),
                                Value::Bool(b) => Some(b.to_string()),
                                _ => None,
                            })
                        };

                        // Create a metadata object with all values
                        let mut all_metadata = serde_json::Map::new();
                        for (key, value) in metadata.iter() {
                            let val_json = match value {
                                Value::String(s) => serde_json::json!(s),
                                Value::Uint8(n) => serde_json::json!(n),
                                Value::Uint16(n) => serde_json::json!(n),
                                Value::Uint32(n) => serde_json::json!(n),
                                Value::Uint64(n) => serde_json::json!(n),
                                Value::Int8(n) => serde_json::json!(n),
                                Value::Int16(n) => serde_json::json!(n),
                                Value::Int32(n) => serde_json::json!(n),
                                Value::Int64(n) => serde_json::json!(n),
                                Value::Float32(f) => serde_json::json!(f),
                                Value::Float64(f) => serde_json::json!(f),
                                Value::Bool(b) => serde_json::json!(b),
                                Value::Array(_, _) => serde_json::json!("[Array]"),
                            };
                            all_metadata.insert(key.clone(), val_json);
                        }
                        model_info["gguf_metadata"] = serde_json::json!(all_metadata);

                        // Get architecture
                        let arch = get_meta_string("general.architecture")
                            .unwrap_or_else(|| "llama".to_string());

                        // Update architecture
                        model_info["architecture"] = serde_json::json!(arch.clone());

                        // Detect tool calling format based on architecture and model name
                        let model_name = get_meta_string("general.name").unwrap_or_default().to_lowercase();
                        let tool_format = if arch.contains("mistral") || model_name.contains("mistral") || model_name.contains("devstral") {
                            "mistral"
                        } else if arch.contains("llama") && (model_name.contains("llama-3") || model_name.contains("llama3")) {
                            "llama3"
                        } else if arch.contains("qwen") || model_name.contains("qwen") {
                            "qwen"
                        } else if arch.contains("llama") {
                            // Older llama models don't support tools
                            "unknown"
                        } else {
                            "unknown"
                        };
                        model_info["tool_format"] = serde_json::json!(tool_format);

                        // Core model information
                        if let Some(val) = get_meta_string("general.name") {
                            model_info["general_name"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("general.author") {
                            model_info["author"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("general.version") {
                            model_info["version"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("general.organization") {
                            model_info["organization"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("general.description") {
                            model_info["description"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("general.license") {
                            model_info["license"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("general.url") {
                            model_info["url"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("general.repo_url") {
                            model_info["repo_url"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("general.file_type") {
                            model_info["file_type"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("general.quantization_version") {
                            model_info["quantization_version"] = serde_json::json!(val);
                        }

                        // Context length - try multiple keys
                        let context_keys = vec![
                            format!("{}.context_length", arch),
                            "llama.context_length".to_string(),
                            "context_length".to_string(),
                        ];
                        for key in &context_keys {
                            if let Some(val) = get_meta_string(key) {
                                model_info["context_length"] = serde_json::json!(val);
                                break;
                            }
                        }

                        // Architecture-specific fields
                        if let Some(val) = get_meta_string(&format!("{}.embedding_length", arch)) {
                            model_info["embedding_length"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string(&format!("{}.block_count", arch)) {
                            model_info["block_count"] = serde_json::json!(val.clone());
                            // Use actual block count for layers
                            if let Ok(block_count) = val.parse::<u32>() {
                                model_info["estimated_layers"] = serde_json::json!(block_count);
                            }
                        }
                        if let Some(val) = get_meta_string(&format!("{}.feed_forward_length", arch)) {
                            model_info["feed_forward_length"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string(&format!("{}.attention.head_count", arch)) {
                            model_info["attention_head_count"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string(&format!("{}.attention.head_count_kv", arch)) {
                            model_info["attention_head_count_kv"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string(&format!("{}.attention.layer_norm_rms_epsilon", arch)) {
                            model_info["layer_norm_epsilon"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string(&format!("{}.rope.dimension_count", arch)) {
                            model_info["rope_dimension_count"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string(&format!("{}.rope.freq_base", arch)) {
                            model_info["rope_freq_base"] = serde_json::json!(val);
                        }

                        // Tokenizer information
                        if let Some(val) = get_meta_string("tokenizer.ggml.model") {
                            model_info["tokenizer_model"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("tokenizer.ggml.bos_token_id") {
                            model_info["bos_token_id"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("tokenizer.ggml.eos_token_id") {
                            model_info["eos_token_id"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("tokenizer.ggml.padding_token_id") {
                            model_info["padding_token_id"] = serde_json::json!(val);
                        }
                        if let Some(val) = get_meta_string("tokenizer.chat_template") {
                            model_info["chat_template"] = serde_json::json!(val);

                            // Extract default system prompt from chat template
                            // Look for: {%- set default_system_message = '...' %}
                            if let Some(start_idx) = val.find("set default_system_message = '") {
                                let after_start = &val[start_idx + "set default_system_message = '".len()..];
                                if let Some(end_idx) = after_start.find("' %}") {
                                    let default_prompt = &after_start[..end_idx];
                                    model_info["default_system_prompt"] = serde_json::json!(default_prompt);
                                }
                            }
                        }
                    }
                }
            }
            
            Response::builder()
                .status(StatusCode::OK)
                .header("content-type", "application/json")
                .header("access-control-allow-origin", "*")
                .body(Body::from(model_info.to_string()))
                .unwrap()
        }
        
        (&Method::GET, "/api/browse") => {
            // Parse query parameters for path
            let query = req.uri().query().unwrap_or("");
            let mut browse_path = "/app/models"; // Default path
            
            // Simple query parameter parsing
            for param in query.split('&') {
                if let Some((key, value)) = param.split_once('=') {
                    if key == "path" {
                        // Simple path assignment (assume already decoded by browser)
                        browse_path = value;
                    }
                }
            }
            
            // Security: ensure path is within allowed directories
            let allowed_paths = ["/app/models", "/app"];
            let is_allowed = allowed_paths.iter().any(|&allowed| {
                browse_path.starts_with(allowed)
            });
            
            if !is_allowed {
                return Ok(Response::builder()
                    .status(StatusCode::FORBIDDEN)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"error":"Path not allowed"}"#))
                    .unwrap());
            }
            
            let mut files = Vec::new();
            let current_path = browse_path.to_string();
            let parent_path = if browse_path != "/app/models" && browse_path != "/app" {
                std::path::Path::new(browse_path)
                    .parent()
                    .and_then(|p| p.to_str())
                    .map(|s| s.to_string())
            } else {
                None
            };
            
            match fs::read_dir(browse_path) {
                Ok(entries) => {
                    for entry in entries {
                        if let Ok(entry) = entry {
                            let path = entry.path();
                            if let (Some(name), Some(path_str)) = (
                                path.file_name().and_then(|n| n.to_str()),
                                path.to_str()
                            ) {
                                let is_directory = path.is_dir();
                                let size = if !is_directory {
                                    entry.metadata().ok().map(|m| m.len())
                                } else {
                                    None
                                };
                                
                                files.push(FileItem {
                                    name: name.to_string(),
                                    path: path_str.to_string(),
                                    is_directory,
                                    size,
                                });
                            }
                        }
                    }
                    
                    // Sort: directories first, then files, both alphabetically
                    files.sort_by(|a, b| {
                        match (a.is_directory, b.is_directory) {
                            (true, false) => std::cmp::Ordering::Less,
                            (false, true) => std::cmp::Ordering::Greater,
                            _ => a.name.to_lowercase().cmp(&b.name.to_lowercase()),
                        }
                    });
                }
                Err(e) => {
                    eprintln!("Failed to read directory {}: {}", browse_path, e);
                    return Ok(Response::builder()
                        .status(StatusCode::NOT_FOUND)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Directory not found"}"#))
                        .unwrap());
                }
            }
            
            let response = BrowseFilesResponse {
                files,
                current_path,
                parent_path,
            };
            
            let response_json = match serde_json::to_string(&response) {
                Ok(json) => json,
                Err(_) => r#"{"files":[],"current_path":"/app/models"}"#.to_string(),
            };
            
            Response::builder()
                .status(StatusCode::OK)
                .header("content-type", "application/json")
                .header("access-control-allow-origin", "*")
                .body(Body::from(response_json))
                .unwrap()
        }
        
        (&Method::GET, "/api/model/status") => {
            #[cfg(feature = "docker")]
            {
                if let Some(state) = llama_state {
                    let status = get_model_status(&state);
                    let response_json = match serde_json::to_string(&status) {
                        Ok(json) => json,
                        Err(_) => r#"{"loaded":false,"model_path":null,"last_used":null,"memory_usage_mb":null}"#.to_string(),
                    };

                    Response::builder()
                        .status(StatusCode::OK)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(response_json))
                        .unwrap()
                } else {
                    Response::builder()
                        .status(StatusCode::SERVICE_UNAVAILABLE)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"loaded":false,"model_path":null,"last_used":null,"memory_usage_mb":null}"#))
                        .unwrap()
                }
            }

            #[cfg(not(feature = "docker"))]
            {
                Response::builder()
                    .status(StatusCode::OK)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"loaded":false,"model_path":null,"last_used":null,"memory_usage_mb":null}"#))
                    .unwrap()
            }
        }

        (&Method::GET, "/api/model/history") => {
            // Load config and return model history
            let config = load_config();
            let response_json = match serde_json::to_string(&config.model_history) {
                Ok(json) => json,
                Err(_) => "[]".to_string(),
            };

            Response::builder()
                .status(StatusCode::OK)
                .header("content-type", "application/json")
                .header("access-control-allow-origin", "*")
                .body(Body::from(response_json))
                .unwrap()
        }

        (&Method::POST, "/api/model/load") => {
            println!("[DEBUG] /api/model/load endpoint hit");

            #[cfg(feature = "docker")]
            {
                if let Some(state) = llama_state {
                    // Parse request body
                    let body_bytes = match hyper::body::to_bytes(req.into_body()).await {
                        Ok(bytes) => bytes,
                        Err(e) => {
                            println!("[DEBUG] Failed to read request body: {}", e);
                            return Ok(Response::builder()
                                .status(StatusCode::BAD_REQUEST)
                                .header("content-type", "application/json")
                                .header("access-control-allow-origin", "*")
                                .body(Body::from(r#"{"success":false,"message":"Failed to read request body"}"#))
                                .unwrap());
                        }
                    };

                    println!("[DEBUG] Request body: {}", String::from_utf8_lossy(&body_bytes));

                    let load_request: ModelLoadRequest = match serde_json::from_slice(&body_bytes) {
                        Ok(req) => req,
                        Err(e) => {
                            println!("[DEBUG] JSON parsing error in model/load: {}", e);
                            println!("[DEBUG] Raw body was: {}", String::from_utf8_lossy(&body_bytes));
                            return Ok(Response::builder()
                                .status(StatusCode::BAD_REQUEST)
                                .header("content-type", "application/json")
                                .header("access-control-allow-origin", "*")
                                .body(Body::from(r#"{"success":false,"message":"Invalid JSON format"}"#))
                                .unwrap());
                        }
                    };

                    // Attempt to load the model
                    match load_model(state.clone(), &load_request.model_path).await {
                        Ok(_) => {
                            // Add to model history on successful load
                            add_to_model_history(&load_request.model_path);

                            let status = get_model_status(&state);
                            let response = ModelResponse {
                                success: true,
                                message: format!("Model loaded successfully from {}", load_request.model_path),
                                status: Some(status),
                            };

                            let response_json = match serde_json::to_string(&response) {
                                Ok(json) => json,
                                Err(_) => r#"{"success":true,"message":"Model loaded successfully","status":null}"#.to_string(),
                            };

                            Response::builder()
                                .status(StatusCode::OK)
                                .header("content-type", "application/json")
                                .header("access-control-allow-origin", "*")
                                .body(Body::from(response_json))
                                .unwrap()
                        }
                        Err(e) => {
                            let response = ModelResponse {
                                success: false,
                                message: format!("Failed to load model: {}", e),
                                status: None,
                            };
                            
                            let response_json = match serde_json::to_string(&response) {
                                Ok(json) => json,
                                Err(_) => format!(r#"{{"success":false,"message":"Failed to load model: {}","status":null}}"#, e),
                            };
                            
                            Response::builder()
                                .status(StatusCode::INTERNAL_SERVER_ERROR)
                                .header("content-type", "application/json")
                                .header("access-control-allow-origin", "*")
                                .body(Body::from(response_json))
                                .unwrap()
                        }
                    }
                } else {
                    Response::builder()
                        .status(StatusCode::SERVICE_UNAVAILABLE)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"success":false,"message":"LLaMA state not available"}"#))
                        .unwrap()
                }
            }
            
            #[cfg(not(feature = "docker"))]
            {
                Response::builder()
                    .status(StatusCode::SERVICE_UNAVAILABLE)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"success":false,"message":"Model loading not available (docker feature not enabled)"}"#))
                    .unwrap()
            }
        }
        
        (&Method::POST, "/api/model/unload") => {
            #[cfg(feature = "docker")]
            {
                if let Some(state) = llama_state {
                    match unload_model(state.clone()).await {
                        Ok(_) => {
                            let status = get_model_status(&state);
                            let response = ModelResponse {
                                success: true,
                                message: "Model unloaded successfully".to_string(),
                                status: Some(status),
                            };
                            
                            let response_json = match serde_json::to_string(&response) {
                                Ok(json) => json,
                                Err(_) => r#"{"success":true,"message":"Model unloaded successfully","status":null}"#.to_string(),
                            };
                            
                            Response::builder()
                                .status(StatusCode::OK)
                                .header("content-type", "application/json")
                                .header("access-control-allow-origin", "*")
                                .body(Body::from(response_json))
                                .unwrap()
                        }
                        Err(e) => {
                            let response = ModelResponse {
                                success: false,
                                message: format!("Failed to unload model: {}", e),
                                status: None,
                            };
                            
                            let response_json = match serde_json::to_string(&response) {
                                Ok(json) => json,
                                Err(_) => format!(r#"{{"success":false,"message":"Failed to unload model: {}","status":null}}"#, e),
                            };
                            
                            Response::builder()
                                .status(StatusCode::INTERNAL_SERVER_ERROR)
                                .header("content-type", "application/json")
                                .header("access-control-allow-origin", "*")
                                .body(Body::from(response_json))
                                .unwrap()
                        }
                    }
                } else {
                    Response::builder()
                        .status(StatusCode::SERVICE_UNAVAILABLE)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"success":false,"message":"LLaMA state not available"}"#))
                        .unwrap()
                }
            }
            
            #[cfg(not(feature = "docker"))]
            {
                Response::builder()
                    .status(StatusCode::SERVICE_UNAVAILABLE)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"success":false,"message":"Model unloading not available (docker feature not enabled)"}"#))
                    .unwrap()
            }
        }
        
        (&Method::POST, "/api/upload") => {
            // Extract headers before consuming the request body
            let content_disposition = req.headers().get("content-disposition")
                .and_then(|h| h.to_str().ok())
                .unwrap_or("")
                .to_string();
                
            let query = req.uri().query().unwrap_or("").to_string();

            // Handle file upload
            let body_bytes = match hyper::body::to_bytes(req.into_body()).await {
                Ok(bytes) => bytes,
                Err(_) => {
                    return Ok(Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"success":false,"message":"Failed to read request body"}"#))
                        .unwrap());
                }
            };
            
            let filename = if content_disposition.contains("filename=") {
                content_disposition
                    .split("filename=")
                    .nth(1)
                    .and_then(|s| s.split(';').next())
                    .map(|s| s.trim_matches('"'))
                    .unwrap_or("uploaded_model.gguf")
            } else {
                // Try to get filename from query parameter
                let mut filename = "uploaded_model.gguf";
                for param in query.split('&') {
                    if let Some((key, value)) = param.split_once('=') {
                        if key == "filename" {
                            filename = value;
                            break;
                        }
                    }
                }
                filename
            };

            // Ensure the filename ends with .gguf
            let filename = if filename.ends_with(".gguf") {
                filename.to_string()
            } else {
                format!("{}.gguf", filename)
            };

            // Save file to models directory
            let file_path = format!("/app/models/{}", filename);
            match fs::write(&file_path, &body_bytes) {
                Ok(_) => {
                    let response = serde_json::json!({
                        "success": true,
                        "message": "File uploaded successfully",
                        "file_path": file_path
                    });
                    
                    Response::builder()
                        .status(StatusCode::OK)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(response.to_string()))
                        .unwrap()
                }
                Err(e) => {
                    let response = serde_json::json!({
                        "success": false,
                        "message": format!("Failed to save file: {}", e)
                    });
                    
                    Response::builder()
                        .status(StatusCode::INTERNAL_SERVER_ERROR)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(response.to_string()))
                        .unwrap()
                }
            }
        }
        
        (&Method::GET, "/api/conversations") => {
            // Fetch conversation files from assets/conversations directory
            let conversations_dir = "assets/conversations";
            let mut conversations = Vec::new();

            match fs::read_dir(conversations_dir) {
                Ok(entries) => {
                    for entry in entries {
                        if let Ok(entry) = entry {
                            let path = entry.path();
                            if path.is_file() && path.extension().map_or(false, |ext| ext == "txt") {
                                if let Some(filename) = path.file_name().and_then(|n| n.to_str()) {
                                    // Extract timestamp from filename (chat_YYYY-MM-DD-HH-mm-ss-SSS.txt)
                                    if filename.starts_with("chat_") && filename.ends_with(".txt") {
                                        let timestamp_part = &filename[5..filename.len()-4]; // Remove "chat_" and ".txt"

                                        conversations.push(ConversationFile {
                                            name: filename.to_string(),
                                            display_name: format!("Chat {}", timestamp_part),
                                            timestamp: timestamp_part.to_string(),
                                        });
                                    }
                                }
                            }
                        }
                    }
                }
                Err(e) => {
                    eprintln!("Failed to read conversations directory: {}", e);
                }
            }

            // Sort conversations by timestamp (newest first)
            conversations.sort_by(|a, b| b.timestamp.cmp(&a.timestamp));

            let response = ConversationsResponse { conversations };
            let response_json = match serde_json::to_string(&response) {
                Ok(json) => json,
                Err(_) => r#"{"conversations":[]}"#.to_string(),
            };

            Response::builder()
                .status(StatusCode::OK)
                .header("content-type", "application/json")
                .header("access-control-allow-origin", "*")
                .body(Body::from(response_json))
                .unwrap()
        }

        (&Method::DELETE, path) if path.starts_with("/api/conversations/") => {
            // Extract filename from path
            let filename = &path["/api/conversations/".len()..];

            // Validate filename to prevent path traversal
            if filename.contains("..") || filename.contains("/") || filename.contains("\\") {
                return Ok(Response::builder()
                    .status(StatusCode::BAD_REQUEST)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"error":"Invalid filename"}"#))
                    .unwrap());
            }

            // Only allow deleting .txt files that start with "chat_"
            if !filename.starts_with("chat_") || !filename.ends_with(".txt") {
                return Ok(Response::builder()
                    .status(StatusCode::BAD_REQUEST)
                    .header("content-type", "application/json")
                    .header("access-control-allow-origin", "*")
                    .body(Body::from(r#"{"error":"Invalid conversation file"}"#))
                    .unwrap());
            }

            let file_path = format!("assets/conversations/{}", filename);

            match fs::remove_file(&file_path) {
                Ok(_) => {
                    println!("Deleted conversation file: {}", filename);
                    Response::builder()
                        .status(StatusCode::OK)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"success":true}"#))
                        .unwrap()
                }
                Err(e) => {
                    eprintln!("Failed to delete conversation file: {}", e);
                    Response::builder()
                        .status(StatusCode::INTERNAL_SERVER_ERROR)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Failed to delete conversation"}"#))
                        .unwrap()
                }
            }
        }

        (&Method::OPTIONS, _) => {
            Response::builder()
                .status(StatusCode::OK)
                .header("access-control-allow-origin", "*")
                .header("access-control-allow-methods", "GET, POST, DELETE, OPTIONS")
                .header("access-control-allow-headers", "content-type")
                .body(Body::empty())
                .unwrap()
        }
        
        (&Method::GET, "/") => {
            // Serve the main index.html from the built frontend
            match std::fs::read_to_string("./dist/index.html") {
                Ok(content) => Response::builder()
                    .status(StatusCode::OK)
                    .header("content-type", "text/html")
                    .body(Body::from(content))
                    .unwrap(),
                Err(_) => {
                    // Fallback HTML if dist files aren't found
                    let html = r#"<!DOCTYPE html>
<html>
<head><title>LLaMA Chat Web</title></head>
<body>
<h1>🦙 LLaMA Chat Web Server</h1>
<p>Web server is running successfully!</p>
<p>Frontend files not found. API endpoints:</p>
<ul>
<li>GET /health - Health check</li>
<li>POST /api/chat - Chat endpoint</li>
<li>GET /api/config - Configuration</li>
</ul>
</body>
</html>"#;
                    Response::builder()
                        .status(StatusCode::OK)
                        .header("content-type", "text/html")
                        .body(Body::from(html))
                        .unwrap()
                }
            }
        }
        
        (&Method::GET, path) if path.starts_with("/assets/") || path.ends_with(".svg") || path.ends_with(".ico") || path.ends_with(".png") => {
            // Serve static assets (JS, CSS, etc.)
            let file_path = format!("./dist{}", path);
            match std::fs::read(&file_path) {
                Ok(content) => {
                    let content_type = if path.ends_with(".js") {
                        "application/javascript"
                    } else if path.ends_with(".css") {
                        "text/css"
                    } else if path.ends_with(".png") {
                        "image/png"
                    } else if path.ends_with(".jpg") || path.ends_with(".jpeg") {
                        "image/jpeg"
                    } else if path.ends_with(".svg") {
                        "image/svg+xml"
                    } else {
                        "application/octet-stream"
                    };
                    
                    Response::builder()
                        .status(StatusCode::OK)
                        .header("content-type", content_type)
                        .header("cache-control", "public, max-age=31536000") // 1 year cache
                        .body(Body::from(content))
                        .unwrap()
                }
                Err(_) => {
                    Response::builder()
                        .status(StatusCode::NOT_FOUND)
                        .body(Body::from("Asset not found"))
                        .unwrap()
                }
            }
        }

        (&Method::POST, "/api/tools/execute") => {
            // Parse request body
            let body_bytes = match hyper::body::to_bytes(req.into_body()).await {
                Ok(bytes) => bytes,
                Err(_) => {
                    return Ok(Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Failed to read request body"}"#))
                        .unwrap());
                }
            };

            #[derive(serde::Deserialize)]
            struct ToolExecuteRequest {
                tool_name: String,
                arguments: serde_json::Value,
            }

            let request: ToolExecuteRequest = match serde_json::from_slice(&body_bytes) {
                Ok(req) => req,
                Err(e) => {
                    println!("JSON parsing error: {}", e);
                    return Ok(Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .header("content-type", "application/json")
                        .header("access-control-allow-origin", "*")
                        .body(Body::from(r#"{"error":"Invalid JSON format"}"#))
                        .unwrap());
                }
            };

            // Execute tool based on name
            let result = match request.tool_name.as_str() {
                "bash" | "shell" | "command" => {
                    // Extract command from arguments
                    let command = request.arguments.get("command")
                        .and_then(|v| v.as_str())
                        .unwrap_or("");

                    if command.is_empty() {
                        return Ok(Response::builder()
                            .status(StatusCode::BAD_REQUEST)
                            .header("content-type", "application/json")
                            .header("access-control-allow-origin", "*")
                            .body(Body::from(r#"{"error":"Command is required"}"#))
                            .unwrap());
                    }

                    // Execute command (with timeout for safety)
                    let output = if cfg!(target_os = "windows") {
                        std::process::Command::new("cmd")
                            .args(["/C", command])
                            .output()
                    } else {
                        std::process::Command::new("sh")
                            .arg("-c")
                            .arg(command)
                            .output()
                    };

                    match output {
                        Ok(output) => {
                            let stdout = String::from_utf8_lossy(&output.stdout).to_string();
                            let stderr = String::from_utf8_lossy(&output.stderr).to_string();
                            let combined = if !stderr.is_empty() {
                                format!("{}\nSTDERR:\n{}", stdout, stderr)
                            } else {
                                stdout
                            };

                            serde_json::json!({
                                "success": true,
                                "result": combined,
                                "exit_code": output.status.code()
                            })
                        }
                        Err(e) => {
                            serde_json::json!({
                                "success": false,
                                "error": format!("Failed to execute command: {}", e)
                            })
                        }
                    }
                }
                _ => {
                    serde_json::json!({
                        "success": false,
                        "error": format!("Unknown tool: {}", request.tool_name)
                    })
                }
            };

            Response::builder()
                .status(StatusCode::OK)
                .header("content-type", "application/json")
                .header("access-control-allow-origin", "*")
                .body(Body::from(result.to_string()))
                .unwrap()
        }

        _ => {
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .body(Body::from("Not Found"))
                .unwrap()
        }
    };
    
    Ok(response)
}

#[tokio::main]
async fn main() -> std::io::Result<()> {
    // Create shared LLaMA state
    #[cfg(feature = "docker")]
    let llama_state: SharedLlamaState = Arc::new(Mutex::new(None));
    
    // Note: ConversationLogger will be created per chat request, not globally
    
    // Create HTTP service
    let make_svc = make_service_fn({
        #[cfg(feature = "docker")]
        let llama_state = llama_state.clone();
        
        move |_conn| {
            #[cfg(feature = "docker")]
            let llama_state = llama_state.clone();
            
            async move {
                Ok::<_, Infallible>(service_fn(move |req| {
                    #[cfg(feature = "docker")]
                    {
                        handle_request(req, llama_state.clone())
                    }
                    #[cfg(not(feature = "docker"))]
                    {
                        handle_request(req)
                    }
                }))
            }
        }
    });
    
    // Start server
    let addr = SocketAddr::from(([0, 0, 0, 0], 8000));
    let server = Server::bind(&addr).serve(make_svc);
    
    println!("🦙 LLaMA Chat Web Server starting on http://{}", addr);
    println!("Available endpoints:");
    println!("  GET  /health               - Health check");
    println!("  POST /api/chat             - Chat with LLaMA");
    println!("  GET  /api/config           - Get sampler configuration");
    println!("  POST /api/config           - Update sampler configuration");
    println!("  GET  /api/model/status     - Get current model status");
    println!("  POST /api/model/load       - Load a specific model");
    println!("  POST /api/model/unload     - Unload current model");
    println!("  POST /api/upload           - Upload model file");
    println!("  GET  /api/conversations    - List conversation files");
    println!("  POST /api/tools/execute    - Execute tool calls");
    println!("  GET  /api/browse           - Browse model files");
    println!("  GET  /                     - Web interface");
    
    server.await.map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))?;
    
    Ok(())
}