[package]
name = "llama_cpp_chat"
version = "0.1.0"
edition = "2021"
default-run = "llama_chat_app"

# Tauri expects a lib.rs with the lib target
[lib]
name = "llama_cpp_chat"
crate-type = ["staticlib", "cdylib", "rlib"]

# Tauri app binary  
[[bin]]
name = "llama_chat_app"
path = "src/main.rs"

# Keep the original CLI binary for testing (disabled for now)
# [[bin]]
# name = "test"
# path = "src/test.rs"

# Web server binary (for Docker deployment)
[[bin]]
name = "llama_chat_web"
path = "src/main_web.rs"

# Temporary utility to check Gemma template
[[bin]]
name = "check_gemma_template"
path = "check_gemma_template.rs"

# Test binary for debugging model loading and tokenization
[[bin]]
name = "test_model"
path = "src/test_model.rs"

[dependencies]
# Tauri framework  
tauri = { version = "2.0", features = [] }
tauri-plugin-shell = "2.0"
tauri-plugin-dialog = "2.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.0", features = ["full"] }
regex = "1.10"

# LLaMA dependencies - with CUDA GPU acceleration
llama-cpp-2 = { version = "0.1.122", features = ["cuda"] }
anyhow = "1.0"
uuid = { version = "1.0", features = ["v4"] }

# Web server dependencies
hyper = { version = "0.14", features = ["full"] }
urlencoding = "2.1"
async-stream = "0.3"
tokio-tungstenite = "0.21"
futures-util = "0.3"
walkdir = "2.5"  # For recursive directory listing

# GGUF file parser for reading model metadata
gguf-llms = "0.0.2"

# WebSocket support
sha1 = "0.10"
base64 = "0.21"

# Logging
log = "0.4"
log4rs = "1.2"
chrono = "0.4"
lazy_static = "1.4"

# SQLite database
rusqlite = { version = "0.30", features = ["bundled"] }

# System monitoring (optional - requires rebuild)
# sysinfo = "0.30"
# nvml-wrapper = { version = "0.9", optional = true }

[features]
default = []
mock = []  # Mock implementation for E2E tests

[build-dependencies]
tauri-build = { version = "2.0", features = [] }
cmake = "0.1"

