[package]
name = "llama_cpp_chat"
version = "0.1.0"
edition = "2021"
default-run = "llama_chat_app"

# Tauri expects a lib.rs with the lib target
[lib]
name = "llama_cpp_chat"
crate-type = ["staticlib", "cdylib", "rlib"]

# Tauri app binary  
[[bin]]
name = "llama_chat_app"
path = "src/main.rs"

# Keep the original CLI binary for testing (disabled for now)
# [[bin]]
# name = "test"
# path = "src/test.rs"

# Web server binary (for Docker deployment)
[[bin]]
name = "llama_chat_web"
path = "src/main_web.rs"

# Temporary utility to check Gemma template
[[bin]]
name = "check_gemma_template"
path = "check_gemma_template.rs"

# Test binary for debugging model loading and tokenization
[[bin]]
name = "test_model"
path = "src/test_model.rs"

# Test binary for vision/mmproj loading
[[bin]]
name = "test_vision"
path = "src/test_vision.rs"

[lints.clippy]
all = "warn"
uninlined_format_args = "warn"
too_many_arguments = "warn"

[profile.dev]
debug = 0
incremental = true

[dependencies]
# Tauri framework
tauri = { version = "2.0", features = [] }
tauri-plugin-shell = "2.0"
tauri-plugin-dialog = "2.0"
tauri-plugin-window-state = "2"
tauri-plugin-single-instance = "2"
tauri-plugin-notification = "2"
tauri-plugin-clipboard-manager = "2"
tauri-plugin-opener = "2"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.0", features = ["full"] }
regex = "1.10"

# LLaMA dependencies - with conditional CUDA GPU acceleration
llama-cpp-2 = { version = "=0.1.122", features = ["mtmd"] }
llama-cpp-sys-2 = { version = "=0.1.122" }  # Pin to match HuggingFace mmproj GGUF format
anyhow = "1.0"
uuid = { version = "1.0", features = ["v4"] }

# Web server dependencies
hyper = { version = "0.14", features = ["full"] }
urlencoding = "2.1"
async-stream = "0.3"
tokio-tungstenite = "0.21"
futures-util = "0.3"
walkdir = "2.5"  # For recursive directory listing

# GGUF file parser for reading model metadata
gguf-llms = "0.0.2"

# WebSocket support
sha1 = "0.10"
base64 = "0.21"

# Logging
log = "0.4"
log4rs = "1.2"
chrono = "0.4"
lazy_static = "1.4"

# Jinja2 template engine for chat templates
minijinja = { version = "2.3", features = ["json"] }

# SQLite database
rusqlite = { version = "0.30", features = ["bundled"] }

# Cross-thread channels for worker process IPC
crossbeam-channel = "0.5"

# HTTP client for web fetching (ureq: lightweight, blocking, rustls for TLS)
ureq = { version = "2.9", default-features = false, features = ["tls"] }

# HTML to text conversion
html2text = "0.12"

# Native OS file dialogs (folder picker for web mode)
rfd = "0.15"

# Headless Chrome for web search (bypasses CAPTCHAs)
headless_chrome = "1.0"
encoding_rs = "0.8.35"

# System monitoring (optional - requires rebuild)
# sysinfo = "0.30"
# nvml-wrapper = { version = "0.9", optional = true }

[features]
default = []
mock = []  # Mock implementation for E2E tests
cuda = ["llama-cpp-2/cuda"]  # CUDA support feature
metal = ["llama-cpp-2/metal"]  # Metal support feature for macOS

[build-dependencies]
tauri-build = { version = "2.0", features = [] }
cmake = "0.1"
ureq = { version = "2.9", default-features = false, features = ["tls"] }
zip = { version = "8.1", default-features = false, features = ["deflate"] }
tar = "0.4"
flate2 = "1.1"
