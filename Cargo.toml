[package]
name = "llama_cpp_chat"
version = "0.1.0"
edition = "2021"
default-run = "llama_chat_app"

# Tauri expects a lib.rs with the lib target
[lib]
name = "llama_cpp_chat"
crate-type = ["staticlib", "cdylib", "rlib"]

# Tauri app binary  
[[bin]]
name = "llama_chat_app"
path = "src/main.rs"

# Keep the original CLI binary for testing (disabled for now)
# [[bin]]
# name = "test"
# path = "src/test.rs"

# Web server binary (for Docker deployment)
[[bin]]
name = "llama_chat_web"
path = "src/main_web.rs"

[dependencies]
# Tauri framework  
tauri = { version = "2.0", features = [] }
tauri-plugin-shell = "2.0"
tauri-plugin-dialog = "2.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.0", features = ["full"] }

# LLaMA dependencies - with CUDA GPU acceleration
llama-cpp-2 = { version = "0.1.122", features = ["cuda"] }
anyhow = "1.0"
uuid = { version = "1.0", features = ["v4"] }

# Web server dependencies
hyper = { version = "0.14", features = ["full"] }
urlencoding = "2.1"
async-stream = "0.3"
tokio-tungstenite = "0.21"
futures-util = "0.3"

# GGUF file parser for reading model metadata
gguf-llms = "0.0.2"

[features]
default = []
mock = []  # Mock implementation for E2E tests

[build-dependencies]
tauri-build = { version = "2.0", features = [] }
cmake = "0.1"

