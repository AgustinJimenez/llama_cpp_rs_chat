# Backend-only Docker build for LLaMA Chat (CLI version)
FROM rust:1.82-bullseye AS builder

# Install system dependencies needed for llama-cpp-2
RUN apt-get update && apt-get install -y \
    cmake \
    build-essential \
    pkg-config \
    libssl-dev \
    git \
    clang \
    libclang-dev \
    && rm -rf /var/lib/apt/lists/*

# Set the working directory
WORKDIR /app

# Copy Cargo files first for better caching
COPY Cargo.toml Cargo.lock ./

# Create dummy source to cache dependencies
RUN mkdir src && echo "fn main() {}" > src/main.rs
RUN echo 'pub fn hello() {}' > src/lib.rs

# Enable llama-cpp-2 for Docker build
RUN sed -i 's/# llama-cpp-2 = "0.1.122"/llama-cpp-2 = "0.1.122"/' Cargo.toml

# Re-enable the test binary for CLI usage
RUN sed -i 's/# \[\[bin\]\]/[[bin]]/' Cargo.toml
RUN sed -i 's/# name = "test"/name = "test"/' Cargo.toml  
RUN sed -i 's/# path = "src\/test.rs"/path = "src\/test.rs"/' Cargo.toml

# Build dependencies (cached layer)
RUN cargo build --release --bin test
RUN rm -rf src

# Copy actual source code
COPY src ./src

# Remove dummy binary and rebuild with real source
RUN rm ./target/release/deps/test*
RUN cargo build --release --bin test

# Runtime image
FROM debian:bullseye-slim

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    ca-certificates \
    libssl1.1 \
    libgomp1 \
    libomp5 \
    && rm -rf /var/lib/apt/lists/*

# Create app user
RUN useradd -m -u 1000 appuser

# Set working directory
WORKDIR /app

# Copy the built backend binary
COPY --from=builder /app/target/release/test ./llama-chat-cli

# Create directories for conversations and models
RUN mkdir -p /app/assets/conversations /app/models
RUN chown -R appuser:appuser /app

# Switch to app user
USER appuser

# Default command
CMD ["./llama-chat-cli"]