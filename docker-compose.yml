version: '3.8'

services:
  # Main web server application service
  llama-chat-app:
    build:
      context: .
      dockerfile: Dockerfile.web
    container_name: llama-chat-web-app
    ports:
      - "8000:3000"
    volumes:
      # Mount model directory (using your existing LM Studio path)
      - ~/.lmstudio/models:/app/models:ro
      # Mount conversations for persistence
      - ./assets/conversations:/app/assets/conversations
      # Optional: mount config file
      - ./config:/app/config
    environment:
      - RUST_LOG=info
      - MODEL_PATH=/app/models/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf
      - LLAMA_CONTEXT_SIZE=32768
      - LLAMA_SAMPLER_TYPE=Greedy
      - TEST_MODE=${TEST_MODE:-false}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Legacy CLI service (for testing/debugging)
  llama-chat-cli:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llama-chat-cli
    volumes:
      - ~/.lmstudio/models:/app/models:ro
      - ./assets/conversations:/app/assets/conversations
    environment:
      - RUST_LOG=debug
      - MODEL_PATH=/app/models/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf
    stdin_open: true
    tty: true
    command: ["./test"]
    profiles:
      - cli

  # Development service with live reload
  llama-chat-dev:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: llama-chat-dev
    ports:
      - "3000:3000"
      - "5173:5173"  # Vite dev server
    volumes:
      - .:/app
      - ~/.lmstudio/models:/app/models:ro
      - node_modules:/app/node_modules
    environment:
      - RUST_LOG=debug
      - NODE_ENV=development
    profiles:
      - dev

  # Optional: Reverse proxy for production
  nginx:
    image: nginx:alpine
    container_name: llama-chat-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - llama-chat-app
    restart: unless-stopped
    profiles:
      - production

networks:
  default:
    name: llama-chat-network

volumes:
  node_modules: